{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 12345\n",
      "Samples: 100%|██████████| 2460/2460 [00:30<00:00, 79.69it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "import functools\n",
    "import glob\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import importlib as imp\n",
    "\n",
    "from tqdm import tqdm\n",
    "tprint = tqdm.write\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.pool import radius_graph\n",
    "from torch_geometric.transforms import FixedPoints\n",
    "from tqdm import tqdm\n",
    "from typing import Callable, Dict, List, Optional, Union\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "\n",
    "# from torch_geometric.nn.conv import PointNetConv\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.inits import reset\n",
    "from torch_geometric.typing import (\n",
    "    Adj,\n",
    "    OptTensor,\n",
    "    PairOptTensor,\n",
    "    PairTensor\n",
    ")\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "\n",
    "import aegnn\n",
    "# from aegnn.models.networks.my_fuse import MyConvBNReLU\n",
    "\n",
    "\n",
    "pl.seed_everything(12345)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "torch.set_printoptions(precision=6)\n",
    "\n",
    "# path = \"/users/yyang22/thesis/aegnn_project/aegnn_results/training_results/checkpoints/ncars/recognition/20230328183028/epoch=0-step=202.pt\" #fuse\n",
    "# path = '/users/yyang22/thesis/aegnn_project/aegnn_results/training_results/checkpoints/ncars/recognition/20230331161858/epoch=18-step=3856.pt' # fuse, quant test\n",
    "# path = '/users/yyang22/thesis/aegnn_project/aegnn_results/training_results/checkpoints/ncars/recognition/20230331160055/epoch=12-step=2638.pt' # fuse, pos div 32\n",
    "# path = '/users/yyang22/thesis/aegnn_project/aegnn_results/training_results/checkpoints/ncars/recognition/20230331164223/epoch=11-step=2435.pt' # fuse, pos abs\n",
    "# path = '/users/yyang22/thesis/aegnn_project/aegnn_results/training_results/checkpoints/ncars/recognition/20230331230610/epoch=99-step=20299.pt' # quant test\n",
    "# path = '/users/yyang22/thesis/aegnn_project/aegnn_results/training_results/checkpoints/ncars/recognition/20230331220429/epoch=1-step=405.pt' # quant debug: dpos = 0\n",
    "# path = '/users/yyang22/thesis/aegnn_project/aegnn_results/training_results/checkpoints/ncars/recognition/20230406184251/epoch=17-step=3653.pt' # quant debug\n",
    "# path = '/users/yyang22/thesis/aegnn_project/aegnn_results/training_results/checkpoints/ncars/recognition/20230406014814/epoch=99-step=20299.pt' # quant debug 2\n",
    "# path = '/users/yyang22/thesis/aegnn_project/aegnn_results/training_results/checkpoints/ncars/recognition/20230407154758/epoch=99-step=20299.pt' # quant debug + qLinear\n",
    "path = '/users/yyang22/thesis/aegnn_project/aegnn_results/training_results/checkpoints/ncars/recognition/20230409231150/epoch=99-step=20299.pt' # quant debug + qLinear 2\n",
    "# path = '/users/yyang22/thesis/aegnn_project/aegnn_results/training_results/checkpoints/ncars/recognition/20230410051217/epoch=99-step=20299.pt' # quant debug + qLinear + sum_pool\n",
    "# path = '/users/yyang22/thesis/aegnn_project/aegnn_results/training_results/checkpoints/ncars/recognition/20230410035349/epoch=99-step=20299.pt' # quant debug + qLinear + avg_pool\n",
    "model = torch.load(path).to(device)\n",
    "model.eval()\n",
    "dm = aegnn.datasets.NCars(batch_size=1, shuffle=False)\n",
    "dm.setup()\n",
    "# print(model.model.fuse1.local_nn.weight)\n",
    "# data_loader = dm.val_dataloader(num_workers=1).__iter__()\n",
    "data_loader = dm.val_dataloader(num_workers=1)\n",
    "\n",
    "assert model.model.fuse1.local_nn.bias is None\n",
    "\n",
    "if isinstance(model, pl.LightningModule):\n",
    "    nn_model = model._modules['model']\n",
    "    nn_layers = nn_model._modules\n",
    "elif isinstance(model, torch.nn.Module):\n",
    "    nn_layers = model._modules\n",
    "else:\n",
    "    raise TypeError(f'The type of model is {type(model)}, not a `torch.nn.Module` or a `pl.LightningModule`')\n",
    "\n",
    "from copy import deepcopy\n",
    "unfused_model = deepcopy(model.model)\n",
    "unfused_model = unfused_model.to(model.device)\n",
    "unfused_model.eval()\n",
    "\n",
    "# for key, nn in nn_layers.items():\n",
    "#     if isinstance(nn, aegnn.models.networks.my_fuse.MyConvBNReLU):\n",
    "#         # nn_layers[key].module.running_mean = torch.zeros_like(nn_layers[key].module.running_mean)\n",
    "#         # nn_layers[key].module.running_var = torch.ones_like(nn_layers[key].module.running_var)\n",
    "#         # nn_layers[key].module.bias = torch.nn.Parameter(torch.zeros_like(nn_layers[key].module.bias))\n",
    "#         # nn_layers[key].module.weight = torch.nn.Parameter(torch.ones_like(nn_layers[key].module.weight))\n",
    "#         # nn_layers[key].module.eps = 1e-16\n",
    "#         nn_layers[key].to_fused()\n",
    "#         pass\n",
    "# fused_model = model\n",
    "# fused_model.eval()\n",
    "assert model.model.fused is False\n",
    "assert model.model.quantized is False\n",
    "model.model.to_fused()\n",
    "assert model.model.fused is True\n",
    "assert model.model.quantized is False\n",
    "\n",
    "model.model.debug_logger()\n",
    "\n",
    "fused_model = model\n",
    "fused_model.eval()\n",
    "\n",
    "\n",
    "\n",
    "# num_test_samples = 12\n",
    "num_test_samples = 2460\n",
    "with torch.no_grad():\n",
    "    for i, sample in enumerate(tqdm(data_loader, position=1, desc='Samples', total=num_test_samples)):\n",
    "        torch.cuda.empty_cache()\n",
    "        if i==num_test_samples: break\n",
    "        # tprint(f\"\\nSample {i}, file_id {sample.file_id}:\")\n",
    "\n",
    "        sample = sample.to(model.device)\n",
    "        tot_nodes = sample.num_nodes\n",
    "\n",
    "\n",
    "        unfused_test_sample = sample.clone().detach()\n",
    "        output_unfused = unfused_model.forward(unfused_test_sample)\n",
    "        y_unfused = torch.argmax(output_unfused, dim=-1)\n",
    "        # tprint(f'unfused output = {output_unfused}')\n",
    "        # tprint(f'{unfused_model.fuse1.local_nn.weight}')\n",
    "\n",
    "        fused_test_sample = sample.clone().detach()\n",
    "        output_fused = fused_model.forward(fused_test_sample)\n",
    "        y_fused = torch.argmax(output_fused, dim=-1)\n",
    "        # tprint(f'  fused output = {output_fused}')\n",
    "        # tprint(f'{fused_model.model.fuse1.local_nn.weight}')\n",
    "        # tprint(fused_model.model.fuse1.fused)\n",
    "\n",
    "        diff = torch.allclose(y_unfused, y_fused)\n",
    "        if diff is not True:\n",
    "            print(i)\n",
    "            print(f'unfused output = {output_unfused}')\n",
    "            print(f'  fused output = {output_fused}')\n",
    "        # print(diff)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot x max = 1.0\n",
      "tot y max = 2.5338151454925537\n",
      "tot wx   abs max = 2.4780657291412354\n",
      "tot wx abs max * x max= 2.4780657291412354\n",
      "tot wpos abs max = 4.607081413269043\n",
      "tot b abs max = 3.6142735481262207\n",
      "calibre = True\n",
      "\n",
      "tot x max = 2.5338151454925537\n",
      "tot y max = 6.525791168212891\n",
      "tot wx   abs max = 2.2445249557495117\n",
      "tot wx abs max * x max= 5.687211513519287\n",
      "tot wpos abs max = 1.9054051637649536\n",
      "tot b abs max = 4.104064464569092\n",
      "calibre = True\n",
      "\n",
      "tot x max = 6.525791168212891\n",
      "tot y max = 6.138178825378418\n",
      "tot wx   abs max = 1.696285367012024\n",
      "tot wx abs max * x max= 11.06960391998291\n",
      "tot wpos abs max = 1.0249121189117432\n",
      "tot b abs max = 2.489299774169922\n",
      "calibre = True\n",
      "\n",
      "tot x max = 6.138178825378418\n",
      "tot y max = 9.349170684814453\n",
      "tot wx   abs max = 1.3518762588500977\n",
      "tot wx abs max * x max= 8.29805850982666\n",
      "tot wpos abs max = 1.1923547983169556\n",
      "tot b abs max = 3.853440999984741\n",
      "calibre = True\n",
      "\n",
      "qLinear:\n",
      "tot in max = 10.49801254272461\n",
      "tot out max = 46.62775421142578\n",
      "tot w max = 0.6548753380775452\n",
      "tot w min = -0.6449093818664551\n",
      "calibre = True\n"
     ]
    }
   ],
   "source": [
    "from aegnn.models.networks.my_fuse import MyConvBNReLU, qLinear\n",
    "\n",
    "for block in fused_model.model.children():\n",
    "    if isinstance(block,MyConvBNReLU):\n",
    "\n",
    "        # print(f'x max = {block.obs_x.max_val}')\n",
    "        # print(f'x min = {block.obs_x.min_val}')\n",
    "        # x_scale, _ = block.obs_x.calculate_qparams()\n",
    "        # print(f'x scale = \\n{x_scale}')\n",
    "        tot_x_max = torch.max(block.obs_x.max_val)\n",
    "        print(f'tot x max = {tot_x_max}')\n",
    "\n",
    "        # print(f'y max = {block.obs_y.max_val}')\n",
    "        tot_y_max = torch.max(block.obs_y.max_val)\n",
    "        print(f'tot y max = {tot_y_max}')\n",
    "\n",
    "        # print(f'w max = {block.obs_w.max_val}')\n",
    "        # print(f'w min = {block.obs_w.min_val}')\n",
    "        # # # w_scale, _ = block.obs_w.calculate_qparams()\n",
    "        # # # print(f'w scale = \\n{w_scale}')\n",
    "        # tot_w_max = torch.max(block.obs_w.max_val)\n",
    "        # tot_w_min = torch.min(block.obs_w.min_val)\n",
    "        # print(f'tot w max = {tot_w_max}')\n",
    "        # print(f'tot w min = {tot_w_min}')\n",
    "        tot_wx_max = torch.max(block.obs_w.max_val[:-2])\n",
    "        tot_wpos_max = torch.max(block.obs_w.max_val[-2:])\n",
    "        tot_wx_min = torch.min(block.obs_w.min_val[:-2])\n",
    "        tot_wpos_min = torch.min(block.obs_w.min_val[-2:])\n",
    "        # print(f'tot wx   max = {tot_wx_max}, min = {tot_wx_min}')\n",
    "        # print(f'tot wpos max = {tot_wpos_max}, min = {tot_wpos_min}')\n",
    "        tot_wx_abs_max = torch.maximum(torch.abs(tot_wx_max), torch.abs(tot_wx_min))\n",
    "        tot_wpos_abs_max = torch.maximum(torch.abs(tot_wpos_max), torch.abs(tot_wpos_min))\n",
    "        print(f'tot wx   abs max = {tot_wx_abs_max}')\n",
    "        print(f'tot wx abs max * x max= {tot_wx_abs_max*tot_x_max}')\n",
    "        print(f'tot wpos abs max = {tot_wpos_abs_max}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        tot_b_max = torch.max(block.obs_b.max_val)\n",
    "        tot_b_min = torch.min(block.obs_b.min_val)\n",
    "        tot_b_abs_max = torch.maximum(tot_b_max.abs(), tot_b_min.abs())\n",
    "        # print(f'tot b max = {tot_b_max}')\n",
    "        # print(f'tot b min = {tot_b_min}')\n",
    "        print(f'tot b abs max = {tot_b_abs_max}')\n",
    "        \n",
    "        print(f'calibre = {block.calibre}')\n",
    "        # print(f'x_scale = {block.x_scale}')\n",
    "        # print(f'y_scale = {block.y_scale}')\n",
    "        # print(f'w_scale = {block.w_scale}')\n",
    "        # print(f'b_scale = {block.b_scale}')\n",
    "        # print(f'M = {block.M}')\n",
    "        \n",
    "        print('')\n",
    "\n",
    "        # print(f'w min = {block.local_nn.weight}')\n",
    "\n",
    "    if isinstance(block,qLinear):\n",
    "        print(f'qLinear:')\n",
    "        print(f'tot in max = {block.obs_in.max_val}')\n",
    "        print(f'tot out max = {block.obs_out.max_val}')\n",
    "        print(f'tot w max = {block.obs_w.max_val}')\n",
    "        print(f'tot w min = {block.obs_w.min_val}')\n",
    "        print(f'calibre = {block.calibre}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv: f_dtype:uint8, w_dtype:int8\n",
      "fc: in_dtype:uint8, out_dtype:int8, w_dtype:int8\n"
     ]
    }
   ],
   "source": [
    "assert fused_model.model.quantized is False\n",
    "\n",
    "f_bit = 8\n",
    "conv_w_bit = 8\n",
    "fc_w_bit = 8\n",
    "fc_out_bit = 8\n",
    "\n",
    "fused_model.model.quant(conv_f_dtype=(f_bit,False), conv_w_dtype=(conv_w_bit,True), fc_in_dtype=(f_bit,False), fc_w_dtype=(fc_w_bit,True), fc_out_dtype=(fc_out_bit,True))\n",
    "\n",
    "print(f'conv: f_dtype:{fused_model.model.fuse1.f_dtype}, w_dtype:{fused_model.model.fuse1.w_dtype}')\n",
    "print(f'fc: in_dtype:{fused_model.model.fc.in_dtype}, out_dtype:{fused_model.model.fc.out_dtype}, w_dtype:{fused_model.model.fc.w_dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert fused_model.model.quantized is True\n",
    "assert fused_model.model.fuse1.quantized is True\n",
    "assert fused_model.model.fc.quantized is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for block in fused_model.model.children():\n",
    "    if isinstance(block,MyConvBNReLU):\n",
    "        \n",
    "        print(f'quantized = {block.quantized}')\n",
    "        print(f'x_scale = {block.x_scale}')\n",
    "        print(f'y_scale = {block.y_scale}')\n",
    "        # print(f'w_scale = {block.w_scale}')\n",
    "        print(f'dpos_scale = {block.dpos_scale}')\n",
    "        print(f'1/dpos_scale = {1/block.dpos_scale}')\n",
    "        # print(f'1/round(1/dpos_scale) = {(1/torch.round(1/block.dpos_scale))}')\n",
    "        # print(f'wx_scale = {block.wx_scale}')\n",
    "        # print(f'wpos_scale = {block.wpos_scale}')\n",
    "        print(f'b_scale = {block.b_scale}')\n",
    "        print(f'M = {block.M}')\n",
    "        m = torch.round(block.M * 2**(20))\n",
    "        print(f'm = {m}')\n",
    "        print(f'M<<>> = {2**(-20)*m}')\n",
    "\n",
    "        # print(f'b_new = {block.b_new}')\n",
    "        # print(f'b_quant = {block.b_quant}')\n",
    "\n",
    "        # print(f'w_quant = {block.w_quant}')\n",
    "        \n",
    "        print('')\n",
    "\n",
    "        # print(f'w min = {block.local_nn.weight}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.debug_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start quant test\n",
      "debug:\n",
      "3\n",
      "unfused output = tensor([[ 14.957020, -15.114644]], device='cuda:0')\n",
      "  quant output = tensor([[ 74271., -75538.]], device='cuda:0')\n",
      "unfused_acc = 0.9605691056910569\n",
      "quant_acc = 0.9613821138211383\n",
      "conv: f_dtype:uint8, w_dtype:int8\n",
      "fc: in_dtype:uint8, w_dtype:int8, out_dtype:int8\n"
     ]
    }
   ],
   "source": [
    "quant_model = model\n",
    "print('start quant test')\n",
    "# model.model.debug_logger()\n",
    "# num_test_samples = 2460\n",
    "# num_test_samples = 12\n",
    "\n",
    "unfused_correct = 0\n",
    "quant_correct = 0\n",
    "with torch.no_grad():\n",
    "    # for i, sample in enumerate(tqdm(data_loader, position=1, desc='Samples', total=num_test_samples)):\n",
    "    for i, sample in enumerate(data_loader):\n",
    "        torch.cuda.empty_cache()\n",
    "        if i==num_test_samples: break\n",
    "        # tprint(f\"\\nSample {i}, file_id {sample.file_id}:\")\n",
    "\n",
    "        sample = sample.to(model.device)\n",
    "        tot_nodes = sample.num_nodes\n",
    "\n",
    "\n",
    "        unfused_test_sample = sample.clone().detach()\n",
    "        output_unfused = unfused_model.forward(unfused_test_sample)\n",
    "        y_unfused = torch.argmax(output_unfused, dim=-1)\n",
    "        # print(f'unfused output = {output_unfused}')\n",
    "        # print(f'x = {unfused_test_sample.x}')\n",
    "        # tprint(f'{unfused_model.fuse1.local_nn.weight}')\n",
    "        unfused_hit = torch.allclose(y_unfused, unfused_test_sample.y)\n",
    "        if unfused_hit: unfused_correct += 1\n",
    "\n",
    "        quant_test_sample = sample.clone().detach()\n",
    "        output_quant = quant_model.forward(quant_test_sample)\n",
    "        y_quant = torch.argmax(output_quant, dim=-1)\n",
    "        # print(f'  quant output = {output_quant}')\n",
    "        # print(f'x = {quant_test_sample.x}')\n",
    "        # tprint(f'{quant_model.model.fuse1.local_nn.weight}')\n",
    "        # tprint(quant_model.model.fuse1.fused)\n",
    "\n",
    "        quant_hit = torch.allclose(y_quant, quant_test_sample.y)\n",
    "        if quant_hit: quant_correct += 1\n",
    "\n",
    "        # diff = torch.allclose(y_unfused, y_quant)\n",
    "        # if diff is not True:\n",
    "        #     print(i)\n",
    "        #     print(f'unfused output = {output_unfused}')\n",
    "        #     print(f'  quant output = {output_quant}')\n",
    "            # print(unfused_test_sample.x)\n",
    "            # print(quant_test_sample.x)\n",
    "        if i == 3 :\n",
    "            print('debug:')\n",
    "            print(i)\n",
    "            print(f'unfused output = {output_unfused}')\n",
    "            print(f'  quant output = {output_quant}')\n",
    "\n",
    "unfused_acc = unfused_correct / num_test_samples\n",
    "quant_acc = quant_correct / num_test_samples\n",
    "\n",
    "print(f'unfused_acc = {unfused_acc}')\n",
    "print(f'quant_acc = {quant_acc}')\n",
    "\n",
    "print(f'conv: f_dtype:{fused_model.model.fuse1.f_dtype}, w_dtype:{fused_model.model.fuse1.w_dtype}')\n",
    "print(f'fc: in_dtype:{fused_model.model.fc.in_dtype}, w_dtype:{fused_model.model.fc.w_dtype}, out_dtype:{fused_model.model.fc.out_dtype}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unfused acc = 0.9605691056910569\n",
    "\n",
    "config: conv f_dtype, w_dtype; fc in_dtype, w_dtype, out_dtype\n",
    "\n",
    "uint8, int8; uint8, int8, int8: 0.9617886178861789\n",
    "\n",
    "uint7, int8; uint7, int8, int8: 0.9601626016260163\n",
    "\n",
    "------\n",
    "\n",
    "uint8, int7; uint8, int8, int8: 0.9617886178861789\n",
    "\n",
    "uint8, int6; uint8, int8, int8: 0.9451219512195121\n",
    "\n",
    "uint8, int5; uint8, int8, int8: 0.8902439024390244\n",
    "\n",
    "uint8, int4; uint8, int8, int8: 0.8951219512195122\n",
    "\n",
    "uint8, int3; uint8, int8, int8: 0.7223577235772358\n",
    "\n",
    "uint8, int2; uint8, int8, int8: 0.4739837398373984\n",
    "\n",
    "-------\n",
    "\n",
    "uint8, int8; uint8, int7, int8: 0.9601626016260163\n",
    "\n",
    "uint8, int8; uint8, int6, int8: 0.9609756097560975\n",
    "\n",
    "uint8, int8; uint8, int5, int8: 0.9613821138211383\n",
    "\n",
    "uint8, int8; uint8, int4, int8: 0.958130081300813\n",
    "\n",
    "uint8, int8; uint8, int3, int8: 0.9508130081300813\n",
    "\n",
    "uint8, int8; uint8, int2, int8: 0.598780487804878\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.debug_dqy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model.debug_qy['fuse4'].max()\n",
    "# model.model.debug_dqy['fuse4'].max()\n",
    "# model.model.debug_y['fuse4'].max()\n",
    "# model.model.debug_y['fuse4'].max() / model.model.fuse4.y_scale\n",
    "model.model.fuse4.y_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_qin = model.model.debug_fc['qin']\n",
    "fc_in = model.model.debug_fc['in']\n",
    "print(fc_in.max())\n",
    "print(fc_qin.max())\n",
    "print(fc_in.max()/fc_qin.max())\n",
    "\n",
    "\n",
    "\n",
    "fc_qout = model.model.debug_qy['qout']\n",
    "fc_out = model.model.debug_fc['out']\n",
    "print(fc_out.max())\n",
    "print(fc_qout.max())\n",
    "print(fc_out.min())\n",
    "print(fc_qout.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nn in quant_model.model.children():\n",
    "    if isinstance(nn, MyConvBNReLU):\n",
    "        w_quant = nn.local_nn.weight\n",
    "        b_quant = nn.b_quant\n",
    "        print(w_quant)\n",
    "        print(b_quant)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aegnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9439450e489ce535473a2847795b2c81cbeeccb2f39d71287859ebd0392d6b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
