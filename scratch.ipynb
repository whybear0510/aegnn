{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import aegnn\n",
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning.metrics.functional as pl_metrics\n",
    "\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.utils import subgraph\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from torch.nn import Linear\n",
    "from torch.nn.functional import elu\n",
    "from torch_geometric.nn.conv import SplineConv\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "from torch_geometric.transforms import Cartesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 4000 samples, wrt their own events/nodes:\n",
    "```\n",
    "acc_Mflop_per_ev = 134.74224981457488, avg_Mflop_per_ev = 0.03368556245364372\n",
    "dense: std acc_Mflop_per_ev = 1170.527851753108, std avg_Mflop_per_ev = 0.29263196293827703\n",
    "```\n",
    "\n",
    "For 100 samples, wrt their own events/nodes:\n",
    "```\n",
    "acc_Mflop_per_ev = 3.414027443525552, avg_Mflop_per_ev = 0.034140274435255524\n",
    "dense: std acc_Mflop_per_ev = 28.595094207357473, std avg_Mflop_per_ev = 0.28595094207357474\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrected formula for calculating FLOPS (according to updated paper), leading to:\n",
    "\n",
    "For 4000 samples, wrt their own events/nodes:\n",
    "```\n",
    "acc_Mflop_per_ev = 240.93893365788432, avg_Mflop_per_ev = 0.06023473341447108\n",
    "dense: std acc_Mflop_per_ev = 2096.876591199398, std avg_Mflop_per_ev = 0.5242191477998495\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = aegnn.datasets.NCars(batch_size=1, shuffle=False)\n",
    "data_module.setup()\n",
    "dm = data_module.train_dataset\n",
    "\n",
    "# cnt = 0\n",
    "# for i, dms in enumerate(dm):\n",
    "#     cnt += 1\n",
    "#     print(i)\n",
    "\n",
    "# print(cnt)\n",
    "\n",
    "# num_trials=100\n",
    "# nodes=[]\n",
    "# nodes_cnt=0\n",
    "# for index in tqdm(range(num_trials)):\n",
    "#     sample = dm[index % len(dm)]\n",
    "#     nodes.append(sample.num_nodes)\n",
    "#     nodes_cnt += sample.num_nodes\n",
    "# print(nodes_cnt)\n",
    "\n",
    "print(dm.dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if add async code into train.py\n",
    "```\n",
    "File \"scripts/train.py\", line 97, in <module>\n",
    "    main(arguments)\n",
    "  File \"scripts/train.py\", line 91, in main\n",
    "    trainer.fit(model, datamodule=dm)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 552, in fit\n",
    "    self._run(model)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 922, in _run\n",
    "    self._dispatch()\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 990, in _dispatch\n",
    "    self.accelerator.start_training(self)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 92, in start_training\n",
    "    self.training_type_plugin.start_training(trainer)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 161, in \n",
    "start_training\n",
    "    self._results = trainer.run_stage()\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1000, in run_stage\n",
    "    return self._run_train()\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1049, in _run_train\n",
    "    self.fit_loop.run()\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\n",
    "    self.advance(*args, **kwargs)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 200, in advance\n",
    "    epoch_output = self.epoch_loop.run(train_dataloader)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 118, in run\n",
    "    output = self.on_run_end()\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 235, in on_run_end\n",
    "    self._on_train_epoch_end_hook(processed_outputs)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 276, in _on_train_epoch_end_hook\n",
    "    trainer_hook(processed_epoch_output)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/callback_hook.py\", line 109, in on_train_epoch_end\n",
    "    callback.on_train_epoch_end(self, self.lightning_module)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 310, in on_train_epoch_end  \n",
    "    self.save_checkpoint(trainer)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 386, in save_checkpoint\n",
    "    self._save_none_monitor_checkpoint(trainer, monitor_candidates)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 742, in _save_none_monitor_checkpoint\n",
    "    self._save_model(trainer, filepath)\n",
    "  File \"/users/yyang22/thesis/aegnn_project/aegnn/aegnn/utils/callbacks/checkpoint_full_model.py\", line 14, in _save_model\n",
    "    torch.save(trainer.model, filepath)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/torch/serialization.py\", line 380, in save\n",
    "    _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/torch/serialization.py\", line 589, in _save\n",
    "    pickler.dump(obj)\n",
    "AttributeError: Can't pickle local object 'make_model_asynchronous.<locals>.async_forward'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if I use wandb:\n",
    "```\n",
    "File \"scripts/train.py\", line 97, in <module>\n",
    "    main(arguments)\n",
    "  File \"scripts/train.py\", line 91, in main\n",
    "    trainer.fit(model, datamodule=dm)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 552, in fit\n",
    "    self._run(model)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 922, in _run\n",
    "    self._dispatch()\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 990, in _dispatch\n",
    "    self.accelerator.start_training(self)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 92, in start_training\n",
    "    self.training_type_plugin.start_training(trainer)  \n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 161, in \n",
    "start_training\n",
    "    self._results = trainer.run_stage()\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1000, in run_stage\n",
    "    return self._run_train()\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1049, in _run_train\n",
    "    self.fit_loop.run()\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\n",
    "    self.advance(*args, **kwargs)\n",
    "File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 200, in advance\n",
    "    epoch_output = self.epoch_loop.run(train_dataloader)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 118, in run\n",
    "    output = self.on_run_end()\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 235, in on_run_end\n",
    "    self._on_train_epoch_end_hook(processed_outputs)   \n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 276, in _on_train_e\n",
    "poch_end_hook\n",
    "    trainer_hook(processed_epoch_output)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/callback_hook.py\", line 109, in on_train_epoch_end\n",
    "    callback.on_train_epoch_end(self, self.lightning_module)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 310, in on_train_epoch_end\n",
    "    self.save_checkpoint(trainer)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 386, in save_checkpoint\n",
    "    self._save_none_monitor_checkpoint(trainer, monitor_candidates)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 742, in _save_none_monitor_checkpoint\n",
    "    self._save_model(trainer, filepath)\n",
    "  File \"/users/yyang22/thesis/aegnn_project/aegnn/aegnn/utils/callbacks/checkpoint_full_model.py\", line 14, in _save_model\n",
    "    torch.save(trainer.model, filepath)\n",
    "File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/torch/serialization.py\", line 380, in save\n",
    "    _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/torch/serialization.py\", line 589, in _save\n",
    "    pickler.dump(obj)\n",
    "AttributeError: Can't pickle local object 'Settings._validator_factory.<locals>.helper'\n",
    "```\n",
    "Problems seem to be at logger function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb import sdk as wandb_sdk\n",
    "\n",
    "wandb.init(project=\"aegnn\", entity=\"yyfteam\")\n",
    "# log_settings = wandb.Settings(start_method=\"thread\")\n",
    "log_settings = wandb.Settings(start_method=\"fork\")\n",
    "wandb_sdk.Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aegnn\n",
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "edge_attr = torch_geometric.transforms.Cartesian(cat=False, max_value=10.0)\n",
    "\n",
    "print(type(edge_attr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "initial_lr = 0.5\n",
    "\n",
    "\n",
    "\n",
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "net_1 = model()\n",
    "\n",
    "def LRPolicy(epoch):\n",
    "    if epoch < 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "optimizer_1 = torch.optim.Adam(net_1.parameters(), lr = initial_lr)\n",
    "scheduler_1 = LambdaLR(optimizer_1, lr_lambda=LRPolicy)\n",
    "\n",
    "print(\"init lr\", optimizer_1.defaults['lr'])\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    # train\n",
    "    optimizer_1.zero_grad()\n",
    "    optimizer_1.step()\n",
    "    print(\"lr of %dth epoch: %f\" % (epoch, optimizer_1.param_groups[0]['lr']))\n",
    "    scheduler_1.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "path='../aegnn_results/training_results/latest'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "else:\n",
    "    # clean\n",
    "    try:\n",
    "        shutil.rmtree(path)\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s : %s\" % (path, e.strerror))\n",
    "    \n",
    "    # rebuild\n",
    "    os.makedirs(path)\n",
    "\n",
    "src_model = sorted(glob.glob(r'/space/yyang22/datasets/data/scratch/checkpoints/ncars/recognition/*/*.pt'), key=os.path.getctime)[-1]\n",
    "dst_model = os.path.join(path,'latest_model.pt')\n",
    "\n",
    "src_log = sorted(glob.glob(r'/space/yyang22/datasets/data/scratch/debug/*'), key=os.path.getctime)[-1]\n",
    "dst_log = os.path.join(path,'latest.log')\n",
    "\n",
    "print(src_model,dst_model,src_log,dst_log)\n",
    "try:\n",
    "    shutil.copy2(src_model, dst_model)\n",
    "except IOError as e:\n",
    "    print(\"Unable to copy file. %s\" % e)\n",
    "except:\n",
    "    print(\"Unexpected error:\", sys.exc_info())\n",
    "\n",
    "try:\n",
    "    shutil.copy2(src_log, dst_log)\n",
    "except IOError as e:\n",
    "    print(\"Unable to copy file. %s\" % e)\n",
    "except:\n",
    "    print(\"Unexpected error:\", sys.exc_info())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cmd = 'python3 ../../test_bkgnd.py'\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3, 1], edge_index=[2, 4], edge_attr=[1, 3], pos=[3, 3])\n",
      "Data(x=[2, 1], edge_index=[2, 2], edge_attr=[1, 3], pos=[2, 3])\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.]])\n",
      "tensor([[0, 1, 1, 2, 3, 4],\n",
      "        [1, 0, 2, 1, 4, 3]])\n",
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 3 required positional arguments: 'dataset', 'input_shape', and 'num_outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m g \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     37\u001b[0m \u001b[39m# net1 = GraphRes(2,4)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m# model_file = '/users/yyang22/thesis/aegnn_project/aegnn_results/training_results/checkpoints/ncars/recognition/20221125084923/epoch=99-step=20299.pt'\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m# net1 = torch.load(model_file).to(torch.device('cuda'))\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m net1 \u001b[39m=\u001b[39m aegnn\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mnetworks\u001b[39m.\u001b[39;49mgraph_res\u001b[39m.\u001b[39;49mGraphRes()\n\u001b[1;32m     42\u001b[0m out1 \u001b[39m=\u001b[39m net1(g1)\n\u001b[1;32m     43\u001b[0m out2 \u001b[39m=\u001b[39m net1(g2)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 3 required positional arguments: 'dataset', 'input_shape', and 'num_outputs'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "import aegnn\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "\n",
    "class GraphRes(torch.nn.Module):\n",
    "    def __init__(self, cin, cout):\n",
    "        super(GraphRes, self).__init__()\n",
    "        self.conv1 = GCNConv(cin, cout)\n",
    "\n",
    "    def forward(self, data: torch_geometric.data.Batch) -> torch.Tensor:\n",
    "        data.x = self.conv1(data.x, data.edge_index)\n",
    "        return data\n",
    "\n",
    "edge_attr = torch.tensor([[0.0,0.0,0.0]])\n",
    "\n",
    "edge_index1 = torch.tensor([[0,1,1,2],[1,0,2,1]], dtype=torch.long)\n",
    "x1 = torch.tensor([[1.0],[2.0],[3.0]])\n",
    "pos1 = torch.tensor([0.,0.,0., 1.,1.,0., 2.,0.,0.]).view(3,3)\n",
    "g1 = torch_geometric.data.Data(x=x1, edge_index=edge_index1, pos=pos1, edge_attr=edge_attr)\n",
    "print(g1)\n",
    "\n",
    "edge_index2 = torch.tensor([[0,1],[1,0]], dtype=torch.long)\n",
    "x2 = torch.tensor([[4.0],[5.0]])\n",
    "pos2 = torch.tensor([0.,0.,1., 1.,1.,1.]).view(2,3)\n",
    "g2 = torch_geometric.data.Data(x=x2, edge_index=edge_index2, pos=pos2, edge_attr=edge_attr)\n",
    "print(g2)\n",
    "\n",
    "g = torch_geometric.data.Batch.from_data_list([g1,g2])\n",
    "print(g.x)\n",
    "print(g.edge_index)\n",
    "print(g.batch)\n",
    "\n",
    "g1 = g1.to(torch.device('cuda'))\n",
    "g2 = g2.to(torch.device('cuda'))\n",
    "g = g.to(torch.device('cuda'))\n",
    "net1 = GraphRes(1,4)\n",
    "# model_file = '/users/yyang22/thesis/aegnn_project/aegnn_results/training_results/checkpoints/ncars/recognition/20221125084923/epoch=99-step=20299.pt'\n",
    "# net1 = torch.load(model_file).to(torch.device('cuda'))\n",
    "# net1 = aegnn.models.networks.graph_res.GraphRes('ncars',)\n",
    "\n",
    "out1 = net1(g1)\n",
    "out2 = net1(g2)\n",
    "out = net1(g)\n",
    "\n",
    "# for param in net1.parameters():\n",
    "#     print(param)\n",
    "\n",
    "print(out1.x)\n",
    "print(out2.x)\n",
    "print(out.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "``CUDA_VISIBLE_DEVICES=5 wandb agent --count 4 yyfteam/aegnn/yzqyfzg6``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in preprocessing.py, order of calling:\n",
    "\n",
    "``ncars``\n",
    "\n",
    "-> ``event_dm.py: prepare_data(self)`` --> ``_prepare_dataset(self)``\n",
    "\n",
    "--> ``ncaltech101.py: _prepare_dataset(self)`` ---> ``processing()@static (parallel)``\n",
    "\n",
    "---> \n",
    "\n",
    " + ``ncars.py: load()@static``  : from text, read ``[x,y,t,p(0/1)]``\n",
    "  + ``ncars.py: read_label()@static``  : 0=car, 1=background; data.label = name, data.y = value\n",
    " + ``ncars.py: pre_transform(self)`` \n",
    "\n",
    "   -----> \n",
    "   + ``.util.normalization.py: normalize_time()`` : ``t_new = (ts - torch.min(ts)) * beta``, beta: float = 0.5e-5\n",
    "   + ``ncaltech101.py: sub_sampling()@static``\n",
    "\n",
    "    &nbsp;&nbsp; ------>\n",
    "    \n",
    "    &nbsp;&nbsp; + ``from torch_geometric.transforms import FixedPoints``: ``FixedPoints(num=10000, allow_duplicates=False, replace=False)`` : will shuffle and subsample events in a event stream !\n",
    "    \n",
    "   + ``from torch_geometric.nn.pool import radius_graph`` : add edge_index by radius_graph\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12345\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: all CUDA-capable devices are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [87], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m raw_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/space/yyang22/datasets/data/storage/ncars/training/sequence_0/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[39m# raw_file = '/space/yyang22/datasets/data/storage/ncars/training/sequence_1118/'\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[39m# load x, pos\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m event_data \u001b[39m=\u001b[39m load(raw_file)\n\u001b[1;32m     34\u001b[0m \u001b[39m# load label name and label value\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m (label \u001b[39m:=\u001b[39m read_label(raw_file)) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn [87], line 16\u001b[0m, in \u001b[0;36mload\u001b[0;34m(raw_file)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(raw_file: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Data:\n\u001b[1;32m     15\u001b[0m     events_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(raw_file, \u001b[39m\"\u001b[39m\u001b[39mevents.txt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     events \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(np\u001b[39m.\u001b[39;49mloadtxt(events_file))\u001b[39m.\u001b[39;49mfloat()\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m     17\u001b[0m     x, pos \u001b[39m=\u001b[39m events[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:], events[:, :\u001b[39m3\u001b[39m]\n\u001b[1;32m     18\u001b[0m     \u001b[39mreturn\u001b[39;00m Data(x\u001b[39m=\u001b[39mx, pos\u001b[39m=\u001b[39mpos)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: all CUDA-capable devices are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import aegnn\n",
    "import os\n",
    "from typing import Callable, List, Optional, Union\n",
    "\n",
    "# torch.cuda.set_device(5)\n",
    "\n",
    "pl.seed_everything(12345)\n",
    "\n",
    "def load(raw_file: str) -> Data:\n",
    "    events_file = os.path.join(raw_file, \"events.txt\")\n",
    "    events = torch.from_numpy(np.loadtxt(events_file)).float().cuda()\n",
    "    x, pos = events[:, -1:], events[:, :3]\n",
    "    return Data(x=x, pos=pos)\n",
    "\n",
    "def read_label(raw_file: str) -> Optional[Union[str, List[str]]]:\n",
    "    label_file = os.path.join(raw_file, \"is_car.txt\")\n",
    "    with open(label_file, \"r\") as f:\n",
    "        label_txt = f.read().replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "    return \"car\" if label_txt == \"1\" else \"background\"\n",
    "\n",
    "\n",
    "class_dict = {class_id: i for i, class_id in enumerate([\"car\", \"background\"])}  # here, car=0, background=1. I dont know why...\n",
    "raw_file = '/space/yyang22/datasets/data/storage/ncars/training/sequence_0/'\n",
    "# raw_file = '/space/yyang22/datasets/data/storage/ncars/training/sequence_1118/'\n",
    "\n",
    "# load x, pos\n",
    "event_data = load(raw_file)\n",
    "\n",
    "# load label name and label value\n",
    "if (label := read_label(raw_file)) is not None:\n",
    "    event_data.label = label if isinstance(label, list) else [label]\n",
    "    event_data.y = torch.tensor([class_dict[label] for label in event_data.label])\n",
    "\n",
    "print(event_data)\n",
    "print(event_data.x.T)\n",
    "print(event_data.pos)\n",
    "print(event_data.pos[:,-1])\n",
    "print(event_data.label)\n",
    "print(event_data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [83], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[39mreturn\u001b[39;00m data\n\u001b[1;32m     13\u001b[0m \u001b[39m# event_data = sub_sampling(event_data, 10000, True)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m event_data \u001b[39m=\u001b[39m sub_sampling(event_data, \u001b[39m10000\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(event_data)\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(event_data\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mT)\n",
      "Cell \u001b[0;32mIn [83], line 10\u001b[0m, in \u001b[0;36msub_sampling\u001b[0;34m(data, n_samples, sub_sample)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m key, item \u001b[39min\u001b[39;00m data:\n\u001b[1;32m      9\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mis_tensor(item) \u001b[39mand\u001b[39;00m item\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 10\u001b[0m         data[key] \u001b[39m=\u001b[39m item[sample_idx]\n\u001b[1;32m     11\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "from torch_geometric.transforms import FixedPoints\n",
    "def sub_sampling(data: Data, n_samples: int, sub_sample: bool) -> Data:\n",
    "    if sub_sample:\n",
    "        sampler = FixedPoints(num=n_samples, allow_duplicates=False, replace=False)\n",
    "        return sampler(data)\n",
    "    else:\n",
    "        sample_idx = np.arange(n_samples)\n",
    "        for key, item in data:\n",
    "            if torch.is_tensor(item) and item.size(0) != 1:\n",
    "                data[key] = item[sample_idx]\n",
    "        return data\n",
    "\n",
    "# event_data = sub_sampling(event_data, 10000, True)\n",
    "event_data = sub_sampling(event_data, 10000, False)\n",
    "\n",
    "print(event_data)\n",
    "print(event_data.x.T)\n",
    "print(event_data.pos)\n",
    "print(event_data.pos[:,-1])\n",
    "print(event_data.label)\n",
    "print(event_data.y)\n",
    "\n",
    "print(torch.min(event_data.pos[:,-1]))\n",
    "print(torch.max(event_data.pos[:,-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.0000e+00, 1.9000e+01, 1.4963e-07],\n",
      "        [7.0000e+00, 3.6000e+01, 2.1136e-07],\n",
      "        [6.6000e+01, 1.1000e+01, 3.2841e-07],\n",
      "        ...,\n",
      "        [2.5000e+01, 2.7000e+01, 4.7212e-07],\n",
      "        [3.0000e+00, 7.0000e+00, 4.8742e-07],\n",
      "        [2.0000e+00, 1.1000e+01, 2.3369e-07]], device='cuda:1')\n",
      "tensor(0., device='cuda:1')\n",
      "tensor(79., device='cuda:1')\n",
      "tensor(0., device='cuda:1')\n",
      "tensor(42., device='cuda:1')\n",
      "tensor(0., device='cuda:1')\n",
      "tensor(4.9966e-07, device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "event_data.pos[:, 2] = (event_data.pos[:, 2] - torch.min(event_data.pos[:, 2])) * 0.5e-5\n",
    "print(event_data.pos)\n",
    "print(torch.min(event_data.pos[:,0]))\n",
    "print(torch.max(event_data.pos[:,0]))\n",
    "\n",
    "print(torch.min(event_data.pos[:,1]))\n",
    "print(torch.max(event_data.pos[:,1]))\n",
    "\n",
    "print(torch.min(event_data.pos[:,2]))\n",
    "print(torch.max(event_data.pos[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  28,   59,   70,  ..., 4625, 4827, 4907],\n",
      "        [   0,    0,    0,  ..., 6262, 6262, 6262]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn.pool import radius_graph\n",
    "event_data.edge_index = radius_graph(event_data.pos, r=3.0, max_num_neighbors=32)\n",
    "print(event_data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[6263, 1], pos=[6263, 3], file_id='sequence_0', label=[1], y=[1], edge_index=[2, 190656])\n",
      "tensor([[2.3000e+01, 2.6000e+01, 2.4466e-07],\n",
      "        [3.0000e+01, 3.6000e+01, 1.0063e-07],\n",
      "        [2.0000e+01, 1.7000e+01, 4.7357e-07],\n",
      "        ...,\n",
      "        [4.1000e+01, 9.0000e+00, 3.6295e-08],\n",
      "        [1.9000e+01, 1.9000e+01, 3.9599e-07],\n",
      "        [2.0000e+01, 3.0000e+00, 4.7259e-07]], device='cuda:1')\n",
      "['car']\n",
      "tensor([0], device='cuda:1')\n",
      "tensor(0., device='cuda:1')\n",
      "tensor(79., device='cuda:1')\n",
      "tensor(0., device='cuda:1')\n",
      "tensor(42., device='cuda:1')\n",
      "tensor(0., device='cuda:1')\n",
      "tensor(4.9966e-07, device='cuda:1')\n",
      "tensor([[2739, 5759, 6107,  ..., 3643, 4029, 1174],\n",
      "        [   0,    0,    0,  ..., 6262, 6262, 6262]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "processed = '/space/yyang22/datasets/data/storage/ncars/processed/training/sequence_0'\n",
    "data2 = torch.load(processed).to(torch.device('cuda'))\n",
    "print(data2)\n",
    "print(data2.pos)\n",
    "print(data2.label)\n",
    "print(data2.y)\n",
    "print(torch.min(data2.pos[:,0]))\n",
    "print(torch.max(data2.pos[:,0]))\n",
    "\n",
    "print(torch.min(data2.pos[:,1]))\n",
    "print(torch.max(data2.pos[:,1]))\n",
    "\n",
    "print(torch.min(data2.pos[:,2]))\n",
    "print(torch.max(data2.pos[:,2]))\n",
    "print(data2.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 2])\n",
      "\n",
      "tensor([3, 3, 3])\n",
      "tensor([0, 0, 0, 2, 7, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def fixed_voxel_grid(pos: Tensor, full_shape: Tensor, size: Tensor, batch: Tensor = None) -> Tensor:\n",
    "\n",
    "    # device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    # params and check\n",
    "    node_dims = pos.size(1)\n",
    "    num_nodes = pos.size(0)\n",
    "    assert len(full_shape) == node_dims\n",
    "    assert len(size)==node_dims or len(size)==1\n",
    "\n",
    "    # batch is None when a single sample\n",
    "    if batch is None:\n",
    "        batch = torch.zeros(num_nodes, dtype=torch.long)\n",
    "\n",
    "    # counting how many grids in each dimension, upward ceiling\n",
    "    num_grids = torch.squeeze(torch.ceil(torch.div(full_shape, size)))\n",
    "\n",
    "    # according to node's pos, calculating its idx (x,y,z,...) in grids\n",
    "    idx = torch.div(pos, size, rounding_mode='floor')\n",
    "    # batch is natually the batch_size idx; transposition for later matmul\n",
    "    idx = torch.cat([idx, batch.view(-1,1)], dim=1).T\n",
    "\n",
    "    # calculating accumulated indices: for grids with (A,B,C,..) voxels idx, and point (x,y,z,...)\n",
    "    # the accumulated indices are: (1,A,AB,ABC,...)\n",
    "    acc_idx = torch.ones(node_dims+1, device=device)\n",
    "    for i in range(node_dims):\n",
    "        acc_idx[i+1] = acc_idx[i] * num_grids[i]\n",
    "\n",
    "    # final index is x*1 + y*A + z*AB + ...., which equals to a vector times the idx\n",
    "    cluster = (acc_idx @ idx).type(torch.long)\n",
    "\n",
    "    return cluster\n",
    "\n",
    "\n",
    "pos = torch.tensor([0.1,0.1, 0.2,0.2, 0.3,0.3, 0.1,0.9], dtype=torch.float).view(-1,2) \n",
    "full_shape = torch.ones(2, dtype=torch.float)\n",
    "size = torch.tensor([0.5,0.5], dtype=torch.float).view(-1,2) \n",
    "# batch = torch.tensor([0,0,1,2], dtype=torch.long)\n",
    "batch = None\n",
    "\n",
    "pos1 = torch.tensor([0.1,0.1,1, 0.2,0.2,1, 0.3,0.3,1, 0.1,0.9,1], dtype=torch.float).view(-1,3)\n",
    "pos2 = torch.tensor([0.6,0.6,1, 0.7,0.7,1, 0.8,0.8,1], dtype=torch.float).view(-1,3)\n",
    "\n",
    "print(fixed_voxel_grid(pos1[:, :2], full_shape, size, batch=batch))\n",
    "print('')\n",
    "print(fixed_voxel_grid(pos2[:, :2], full_shape, size, batch=batch))\n",
    "\n",
    "pos3 = torch.cat([pos1, pos2])\n",
    "batch1 = torch.tensor([0,0,0,0,1,1,1], dtype=torch.long)\n",
    "print(fixed_voxel_grid(pos3[:, :2], full_shape, size, batch=batch1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3250, 1], pos=[3250, 3], file_id='sequence_1', label=[1], y=[1], edge_index=[2, 66166])\n",
      "g=DataBatch(x=[7, 1], edge_index=[2, 6], pos=[7, 3], batch=[7], ptr=[3])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'img_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 131\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mout=\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mout\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    129\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m--> 131\u001b[0m net  \u001b[39m=\u001b[39m Net()\n\u001b[1;32m    132\u001b[0m net\u001b[39m.\u001b[39meval()\n\u001b[1;32m    134\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mg1:\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [5], line 81\u001b[0m, in \u001b[0;36mNet.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m0.5\u001b[39m,\u001b[39m0.5\u001b[39m])\n\u001b[1;32m     78\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfull_shape \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m1.0\u001b[39m,\u001b[39m1.0\u001b[39m])\n\u001b[0;32m---> 81\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool \u001b[39m=\u001b[39m MaxPoolingX(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_grid)\n\u001b[1;32m     83\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc \u001b[39m=\u001b[39m  Linear(\u001b[39m4\u001b[39m\u001b[39m*\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_grid), out_features\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'img_shape'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import aegnn\n",
    "import os\n",
    "from typing import Callable, List, Optional, Union\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "from torch_geometric.transforms import Cartesian\n",
    "from aegnn.models.layer import MaxPooling, MaxPoolingX\n",
    "\n",
    "from aegnn.models.layer import MaxPooling, MaxPoolingX\n",
    "from torch_geometric.nn.pool import max_pool_x, voxel_grid, avg_pool_x\n",
    "from torch_geometric.nn import GCNConv, Sequential, global_max_pool,global_mean_pool\n",
    "from torch.nn.functional import elu, relu\n",
    "from torch.nn import Dropout, Linear\n",
    "from torch_cluster import grid_cluster\n",
    "\n",
    "pl.seed_everything(12345)\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "device  = torch.device('cpu')\n",
    "\n",
    "\n",
    "\n",
    "x1 = torch.tensor([1.0, -2.0, 3.0, -4.0], dtype=torch.float, device=device).view(-1,1)\n",
    "x2 = torch.tensor([5.0, -6.0, 7.0,         ], dtype=torch.float, device=device).view(-1,1)\n",
    "\n",
    "edge1 = torch.tensor([0,2,1,3, 2,0,3,1], dtype=torch.long, device=device).view(2,-1)\n",
    "edge2 = torch.tensor([1,2,2,1], dtype=torch.long, device=device).view(2,-1)\n",
    "\n",
    "# in this setting, cluster will give diff result for g1,g2 and g\n",
    "# pos1 = torch.tensor([-0.49,-0.3,0.02, -0.49,-0.1,-0.03, 0.49,-0.1,0.01, 0.49,-0.3,-0.02], dtype=torch.float, device=device).view(-1,3)\n",
    "# pos1 += 0.5\n",
    "# pos2 = torch.tensor([-0.21,-0.49,0.02, -0.21,0.49,-0.03, -0.01,0.49,0.01,              ], dtype=torch.float, device=device).view(-1,3)\n",
    "# pos2 += 0.5\n",
    "\n",
    "# new test\n",
    "pos1 = torch.tensor([0.1,0.1,1, 0.2,0.2,1, 0.3,0.3,1, 0.1,0.9,1], dtype=torch.float, device=device).view(-1,3)\n",
    "pos2 = torch.tensor([0.6,0.6,1, 0.7,0.7,1, 0.8,0.8,1], dtype=torch.float, device=device).view(-1,3)\n",
    "\n",
    "g1 = Data(x=x1, edge_index=edge1, pos=pos1)\n",
    "g2 = Data(x=x2, edge_index=edge2, pos=pos2)\n",
    "g3 = Data(x=x2, edge_index=edge2, pos=pos2)\n",
    "g = torch_geometric.data.Batch.from_data_list([g1,g2])\n",
    "\n",
    "path1 = '/space/yyang22/datasets/data/storage/ncars/processed/training/sequence_0'\n",
    "path2 = '/space/yyang22/datasets/data/storage/ncars/processed/training/sequence_1'\n",
    "\n",
    "aegnn1 = torch.load(path2).to(device)\n",
    "aegnn2 = torch.load(path2).to(device)\n",
    "aegnn_whole = torch_geometric.data.Batch.from_data_list([aegnn1, aegnn2])\n",
    "print(aegnn1)\n",
    "\n",
    "print(f'g={g}')\n",
    "# print(g.x)\n",
    "# print(g.pos)\n",
    "# print(g.edge_index)\n",
    "# print(g.batch)\n",
    "\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(1, 4)\n",
    "        self.norm1 = BatchNorm(in_channels=4)\n",
    "        self.act   = elu\n",
    "\n",
    "        self.batch_size = 2\n",
    "        self.grid_div = 2\n",
    "        self.num_grid = self.grid_div*self.grid_div\n",
    "        # self.size=([1.0/self.grid_div,1.0/self.grid_div])\n",
    "        self.size = torch.tensor([0.5,0.5])\n",
    "        self.full_shape = torch.tensor([1.0,1.0])\n",
    "\n",
    "\n",
    "        self.pool = MaxPoolingX(self.size, size=self.num_grid)\n",
    "\n",
    "        self.fc =  Linear(4*(self.num_grid), out_features=2, bias=False)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x1 = data.x = self.conv1(data.x, data.edge_index)\n",
    "        # x2 = data.x = self.norm1(data.x)\n",
    "        x2 = data.x\n",
    "        # x3 = data.x = self.act(data.x)\n",
    "        x3 = data.x\n",
    "\n",
    "        # print(f'x1=\\n{x1}')\n",
    "        # print(f'x2=\\n{x2}')\n",
    "        # print(f'x3=\\n{x3}')\n",
    "\n",
    "        if data.batch is None:\n",
    "            data.batch = torch.zeros(data.num_nodes)\n",
    "        else:\n",
    "            print(f'data.batch=\\n{data.batch}')\n",
    "        \n",
    "        # end = ((data.batch.max().item() + 1.0)*self.num_grid - 1)\n",
    "        # print(f'end={end}')\n",
    "\n",
    "        # cluster = voxel_grid(data.pos[:, :2], batch=data.batch, size=self.size)\n",
    "        cluster = fixed_voxel_grid(data.pos[:, :2], full_shape=self.full_shape, batch=data.batch, size=self.size)\n",
    "\n",
    "        # pos = torch.cat([data.pos[:, :2], data.batch.unsqueeze(-1).type_as(data.pos[:, :2])], dim=-1)\n",
    "        # size = self.size + [1]\n",
    "        # print(size)\n",
    "        # size = torch.tensor(size, dtype=pos.dtype, device=pos.device)\n",
    "        # start = torch.tensor([0.0,0.0,0.0], dtype=pos.dtype, device=pos.device)\n",
    "        # end = torch.tensor([1.0,1.0,1.0], dtype=pos.dtype, device=pos.device)\n",
    "        # cluster = grid_cluster(pos, size, start, end)\n",
    "\n",
    "        x4_auth, _ = max_pool_x(cluster, data.x, data.batch, size=self.num_grid)\n",
    "\n",
    "        print(f'cluster=\\n{cluster}')\n",
    "        print(f'x4_auth=\\n{x4_auth}')\n",
    "\n",
    "        x4_aegnn = self.pool(data.x, pos=data.pos[:, :2], batch=data.batch)\n",
    "        # print(f'x4_aegnn=\\n{x4_aegnn}')\n",
    "        # print(f'same={torch.allclose(x4_auth, x4_aegnn)}')\n",
    "\n",
    "        x5 = x4_auth.view(-1, self.fc.in_features)\n",
    "        # print(f'x5=\\n{x5}')\n",
    "        out = self.fc(x5)\n",
    "        print(f'out=\\n{out}')\n",
    "\n",
    "        return out\n",
    "\n",
    "net  = Net()\n",
    "net.eval()\n",
    "\n",
    "print('\\ng1:')\n",
    "g1_out = net(g1)\n",
    "print('\\ng2:')\n",
    "g2_out = net(g2)\n",
    "print('\\ng:')\n",
    "g_out = net(g)\n",
    "\n",
    "# print('\\naegnn1:')\n",
    "# aegnn1_out = net(aegnn1)\n",
    "# print('\\naegnn2:')\n",
    "# aegnn2_out = net(aegnn2)\n",
    "# print('\\naegnn_whole:')\n",
    "# aegnn_whole_out = net(aegnn_whole)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 1, 4, 6, 6])\n",
      "tensor([[2.0000, 1.1000],\n",
      "        [4.0000, 3.1000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [5.0000, 5.1000],\n",
      "        [0.0000, 0.0000],\n",
      "        [7.0000, 7.1000],\n",
      "        [0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# from aegnn.models.layer import MaxPooling, MaxPoolingX\n",
    "from torch_geometric.nn.pool import max_pool_x, voxel_grid, avg_pool_x\n",
    "from torch_geometric.nn import GCNConv, Sequential, global_max_pool,global_mean_pool\n",
    "\n",
    "batch_size = 2\n",
    "grid_div = 2\n",
    "num_grid = grid_div*grid_div\n",
    "cluster = voxel_grid(g.pos[:, :2], batch=g.batch, size=([1.0/grid_div,1.0/grid_div]))\n",
    "x, _ = max_pool_x(cluster, g.x, g.batch, size=num_grid)\n",
    "# zero = torch.tensor([0,0,0,0,0,0,0])\n",
    "# x, _ = max_pool_x(zero, g.x, g.batch, size=num_grid)\n",
    "# x, _ = max_pool_x(cluster, g.x, g.batch)\n",
    "\n",
    "\n",
    "# x_new = x.view(batch_size, g.x.shape[1]*num_grid)\n",
    "\n",
    "print(cluster)\n",
    "print(x)\n",
    "# print(x_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.3892, 2.6133],\n",
      "        [8.5804, 4.4811]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Dropout, Linear, ReLU\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "pl.seed_everything(12345)\n",
    "\n",
    "fc = Linear(g.x.shape[1]*num_grid, out_features=2, bias=False)\n",
    "output = fc(x_new)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.5000, -0.5000],\n",
      "        [ 6.0000,  6.1000]])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv, Sequential, global_max_pool,global_mean_pool\n",
    "# gmp = global_max_pool\n",
    "gmp = global_mean_pool\n",
    "x_ori_output = gmp(g.x, batch=g.batch)\n",
    "print(x_ori_output)\n",
    "# fc_ori = Linear(g.x.shape[1], out_features=2, bias=False)\n",
    "# output_ori = fc_ori(x_ori_output)\n",
    "# print(x_ori_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[3, 1], edge_index=[2, 6], pos=[3, 3])\n",
      "tensor([[ 1.],\n",
      "        [ 3.],\n",
      "        [-4.]])\n",
      "tensor([[0, 0, 1, 1, 2, 2],\n",
      "        [1, 2, 0, 2, 0, 1]])\n",
      "tensor([[0.1500, 0.1500, 1.0000],\n",
      "        [0.1000, 0.6000, 1.0000],\n",
      "        [0.6000, 0.6000, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import aegnn\n",
    "import os\n",
    "from typing import Callable, List, Optional, Union\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "from torch_geometric.transforms import Cartesian\n",
    "from aegnn.models.layer import MaxPooling, MaxPoolingX\n",
    "\n",
    "from aegnn.models.layer import MaxPooling, MaxPoolingX\n",
    "from torch_geometric.nn.pool import max_pool_x, voxel_grid, avg_pool_x, max_pool\n",
    "from torch_geometric.nn import GCNConv, Sequential, global_max_pool,global_mean_pool\n",
    "from torch.nn.functional import elu, relu\n",
    "from torch.nn import Dropout, Linear\n",
    "from torch_cluster import grid_cluster\n",
    "\n",
    "pl.seed_everything(12345)\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "device  = torch.device('cpu')\n",
    "\n",
    "\n",
    "\n",
    "x1 = torch.tensor([1.0, -2.0, 3.0, -4.0], dtype=torch.float, device=device).view(-1,1)\n",
    "x2 = torch.tensor([5.0, -6.0, 7.0,         ], dtype=torch.float, device=device).view(-1,1)\n",
    "\n",
    "edge1 = torch.tensor([0,3,1,2,2,3, 3,0,2,1,3,2], dtype=torch.long, device=device).view(2,-1)\n",
    "edge2 = torch.tensor([1,2,2,1], dtype=torch.long, device=device).view(2,-1)\n",
    "\n",
    "pos1 = torch.tensor([0.1,0.1,1, 0.2,0.2,1, 0.1,0.6,1, 0.6,0.6,1], dtype=torch.float, device=device).view(-1,3)\n",
    "pos2 = torch.tensor([0.6,0.6,1, 0.7,0.7,1, 0.8,0.8,1], dtype=torch.float, device=device).view(-1,3)\n",
    "\n",
    "g1 = Data(x=x1, edge_index=edge1, pos=pos1)\n",
    "g2 = Data(x=x2, edge_index=edge2, pos=pos2)\n",
    "g3 = Data(x=x2, edge_index=edge2, pos=pos2)\n",
    "g = torch_geometric.data.Batch.from_data_list([g1,g2])\n",
    "\n",
    "batch_size = 2\n",
    "grid_div = 2\n",
    "num_grid = grid_div*grid_div\n",
    "cluster = voxel_grid(g1.pos[:, :2], batch=g1.batch, size=([1.0/grid_div,1.0/grid_div]))\n",
    "data = max_pool(cluster, g1)\n",
    "\n",
    "print(data)\n",
    "print(data.x)\n",
    "print(data.edge_index)\n",
    "print(data.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[4, 1], edge_index=[2, 6], pos=[4, 3])\n",
      "tensor([1.0000, 1.0000, 0.5831, 0.5831, 0.7071, 0.7071])\n",
      "edge_attr=tensor([], size=(6, 0))\n",
      "Data(x=[4, 1], edge_index=[2, 6], pos=[4, 3], edge_attr=[6, 0], edge_weight=[6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import aegnn\n",
    "import os\n",
    "from typing import Callable, List, Optional, Union\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "from torch_geometric.transforms import Cartesian, Distance\n",
    "from aegnn.models.layer import MaxPooling, MaxPoolingX\n",
    "\n",
    "from aegnn.models.layer import MaxPooling, MaxPoolingX\n",
    "from torch_geometric.nn.pool import max_pool_x, voxel_grid, avg_pool_x, max_pool\n",
    "from torch_geometric.nn import GCNConv, Sequential, global_max_pool,global_mean_pool\n",
    "from torch.nn.functional import elu, relu\n",
    "from torch.nn import Dropout, Linear\n",
    "from torch_cluster import grid_cluster\n",
    "\n",
    "pl.seed_everything(12345)\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "device  = torch.device('cpu')\n",
    "\n",
    "\n",
    "\n",
    "x1 = torch.tensor([1.0, -2.0, 3.0, -4.0], dtype=torch.float, device=device).view(-1,1)\n",
    "x2 = torch.tensor([5.0, -6.0, 7.0,         ], dtype=torch.float, device=device).view(-1,1)\n",
    "\n",
    "edge1 = torch.tensor([0,3,1,2,2,3, 3,0,2,1,3,2], dtype=torch.long, device=device).view(2,-1)\n",
    "edge2 = torch.tensor([1,2,2,1], dtype=torch.long, device=device).view(2,-1)\n",
    "\n",
    "pos1 = torch.tensor([0.1,0.1,1, 0.2,0.2,1, 0.1,0.6,1, 0.6,0.6,1], dtype=torch.float, device=device).view(-1,3)\n",
    "pos2 = torch.tensor([0.6,0.6,1, 0.7,0.7,1, 0.8,0.8,1], dtype=torch.float, device=device).view(-1,3)\n",
    "\n",
    "g1 = Data(x=x1, edge_index=edge1, pos=pos1)\n",
    "g2 = Data(x=x2, edge_index=edge2, pos=pos2)\n",
    "g3 = Data(x=x2, edge_index=edge2, pos=pos2)\n",
    "g = torch_geometric.data.Batch.from_data_list([g1,g2])\n",
    "\n",
    "print(g1)\n",
    "\n",
    "# trans2 = Cartesian()\n",
    "\n",
    "# g1 = trans2(g1)\n",
    "\n",
    "trans = Distance()\n",
    "g1 = trans(g1)\n",
    "g1.edge_weight = g1.edge_attr[:,-1]\n",
    "g1.edge_attr = g1.edge_attr[:, :-1]\n",
    "\n",
    "print(g1.edge_weight)\n",
    "print(f'edge_attr={g1.edge_attr}')\n",
    "print(g1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "CUDA_VISIBLE_DEVICES=2 wandb agent yyfteam/aegnn/l8y4mj6f\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.loadtxt('/users/yyang22/thesis/aegnn_project/aegnn_results/async_accuracy.csv', delimiter=',')\n",
    "plt.plot(data[:,0], data[:,1])\n",
    "\n",
    "plt.savefig('/users/yyang22/thesis/aegnn_project/aegnn_results/async_accuracy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 3), (4, 18), (12, 23), (19, 22), (19, 24), (22, 24)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "# from torch import tensor\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import k_hop_subgraph, remove_self_loops, to_networkx\n",
    "\n",
    "a = torch.tensor([False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,True,False,True]).view(-1,1)\n",
    "b=torch.unique(torch.nonzero(a)[:,0])\n",
    "# print(b)  #tensor([12, 23, 25])\n",
    "idx_update = b\n",
    "module_asy_graph_edge_index = torch.tensor([[ 3,  0, 18, 23,  4, 22, 24, 19, 24, 12, 19, 22],\n",
    "                                            [ 0,  3,  4, 12, 18, 19, 19, 22, 22, 23, 24, 24]])\n",
    "pos  = torch.tensor([[5.4000e+01, 5.2000e+01, 0.0000e+00],\n",
    "        [5.7000e+01, 4.0000e+00, 7.0000e-10],\n",
    "        [5.3000e+01, 3.3000e+01, 7.0500e-10],\n",
    "        [5.4000e+01, 5.1000e+01, 7.4500e-10],\n",
    "        [5.8000e+01, 9.0000e+00, 9.1000e-10],\n",
    "        [5.7000e+01, 2.3000e+01, 9.2500e-10],\n",
    "        [5.0000e+01, 4.7000e+01, 9.3000e-10],\n",
    "        [4.9000e+01, 0.0000e+00, 9.5500e-10],\n",
    "        [1.1000e+02, 5.4000e+01, 9.8000e-10],\n",
    "        [5.1000e+01, 5.4000e+01, 9.8000e-10],\n",
    "        [5.7000e+01, 1.7000e+01, 1.1150e-09],\n",
    "        [4.9000e+01, 5.8000e+01, 1.1200e-09],\n",
    "        [5.7000e+01, 1.3000e+01, 1.1250e-09],\n",
    "        [5.3000e+01, 5.7000e+01, 1.1450e-09],\n",
    "        [5.1000e+01, 4.0000e+01, 1.2400e-09],\n",
    "        [5.0000e+01, 6.2000e+01, 1.2500e-09],\n",
    "        [5.7000e+01, 2.0000e+01, 1.5850e-09],\n",
    "        [5.5000e+01, 4.3000e+01, 1.5900e-09],\n",
    "        [5.8000e+01, 1.0000e+01, 1.6000e-09],\n",
    "        [1.0600e+02, 5.2000e+01, 1.6400e-09],\n",
    "        [5.3000e+01, 6.1000e+01, 1.8250e-09],\n",
    "        [1.0200e+02, 6.3000e+01, 1.8350e-09],\n",
    "        [1.0500e+02, 5.3000e+01, 1.8900e-09],\n",
    "        [5.8000e+01, 1.3000e+01, 2.3500e-09],\n",
    "        [1.0600e+02, 5.2000e+01, 2.3850e-09],\n",
    "        [5.6000e+01, 1.4000e+01, 2.4000e-09]])\n",
    "data = Data(x=torch.zeros(26),edge_index=module_asy_graph_edge_index, pos=pos[:,:2])\n",
    "pos_list = pos[:,:2].tolist()\n",
    "\n",
    "g = to_networkx(data, to_undirected=True)\n",
    "print(g.edges)\n",
    "nx.draw_networkx(g, with_labels=True,  node_size=0, node_shape='.',  font_size=8, font_color='k',  edge_color='b', width=1)\n",
    "# nx.draw_networkx_edges(g, pos = pos_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12, 23, 25])\n",
      "tensor([[23, 12],\n",
      "        [12, 23]])\n",
      "tensor([0, 1, 2])\n",
      "tensor([False, False, False,  True, False, False, False, False, False,  True,\n",
      "        False, False])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nodes_in_subgraph, edges_connected, nodes_new_location, connected_edges_mask = k_hop_subgraph(idx_update, num_hops=1,\n",
    "                                                                edge_index=module_asy_graph_edge_index,\n",
    "                                                                num_nodes=26)\n",
    "print(nodes_in_subgraph)\n",
    "print(edges_connected)\n",
    "print(nodes_new_location)\n",
    "print(connected_edges_mask) # original edge_index --- connected_edges_mask ---> edges_connected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12, 23, 25],\n",
      "        [ 0,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "c = torch.nonzero(a).T\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 39.1152,  35.1283,  59.0085,  34.5254,  31.2570,  77.9295,  90.6091,\n",
      "          56.0357,  36.3456,  49.0102,  49.0918,   8.0623,  31.2570,  31.4006,\n",
      "          38.0132,  84.3090,  69.6348,  76.3217,  14.7648,  20.5913,  74.1080,\n",
      "          38.8330,   7.0711,   3.1623,  65.0000,  62.0725,  67.1193,  22.4722,\n",
      "           2.2361,   5.3852,  43.0465,   9.0554,  95.1893,  62.8172,  27.6586,\n",
      "          21.9317,  20.0250,  39.8497,  33.9706,  34.3657,   5.0000, 103.1213,\n",
      "          10.0000,  47.0425,  26.4764,  29.6142,  67.0075,  38.4187,  51.4782,\n",
      "          13.6015,  85.0000,  68.7314,  36.3593,  52.2015,  71.1758,   0.0000]])\n"
     ]
    }
   ],
   "source": [
    "pos_all = torch.tensor([[6.5000e+01, 7.8000e+01, 0.0000e+00],\n",
    "        [6.9000e+01, 7.8000e+01, 0.0000e+00],\n",
    "        [4.5000e+01, 7.6000e+01, 0.0000e+00],\n",
    "        [7.0000e+01, 6.9000e+01, 5.0000e-12],\n",
    "        [1.0000e+02, 4.4000e+01, 1.0000e-11],\n",
    "        [2.7000e+01, 6.3000e+01, 1.4500e-10],\n",
    "        [1.5000e+01, 5.8000e+01, 1.7500e-10],\n",
    "        [1.0600e+02, 1.9000e+01, 1.9000e-10],\n",
    "        [1.0900e+02, 3.9000e+01, 6.6000e-10],\n",
    "        [1.0300e+02, 2.6000e+01, 7.0500e-10],\n",
    "        [1.0100e+02, 2.6000e+01, 7.0500e-10],\n",
    "        [1.0000e+02, 6.8000e+01, 1.0400e-09],\n",
    "        [1.0000e+02, 4.4000e+01, 1.0700e-09],\n",
    "        [1.0900e+02, 4.4000e+01, 1.0750e-09],\n",
    "        [8.2000e+01, 4.4000e+01, 1.0750e-09],\n",
    "        [2.6000e+01, 4.3000e+01, 1.0750e-09],\n",
    "        [4.7000e+01, 3.5000e+01, 1.0800e-09],\n",
    "        [3.9000e+01, 3.5000e+01, 1.0800e-09],\n",
    "        [9.7000e+01, 6.2000e+01, 1.0850e-09],\n",
    "        [9.4000e+01, 5.7000e+01, 1.4150e-09],\n",
    "        [1.0800e+02, 1.0000e+00, 1.7300e-09],\n",
    "        [9.6000e+01, 3.7000e+01, 1.8750e-09],\n",
    "        [9.9000e+01, 7.0000e+01, 1.9500e-09],\n",
    "        [1.0300e+02, 7.2000e+01, 2.0000e-09],\n",
    "        [1.0400e+02, 1.0000e+01, 2.0150e-09],\n",
    "        [1.0700e+02, 1.3000e+01, 2.0950e-09],\n",
    "        [1.0800e+02, 8.0000e+00, 2.1050e-09],\n",
    "        [8.5000e+01, 6.3000e+01, 2.1150e-09],\n",
    "        [1.0300e+02, 7.3000e+01, 2.3800e-09],\n",
    "        [9.9000e+01, 7.3000e+01, 2.3800e-09],\n",
    "        [1.0200e+02, 3.2000e+01, 2.4000e-09],\n",
    "        [9.5000e+01, 7.4000e+01, 2.4050e-09],\n",
    "        [1.0000e+01, 6.0000e+01, 2.5500e-09],\n",
    "        [4.3000e+01, 6.0000e+01, 2.5500e-09],\n",
    "        [9.8000e+01, 4.8000e+01, 2.5550e-09],\n",
    "        [8.4000e+01, 6.6000e+01, 2.5650e-09],\n",
    "        [8.4000e+01, 7.6000e+01, 2.5800e-09],\n",
    "        [9.2000e+01, 3.7000e+01, 2.5850e-09],\n",
    "        [8.1000e+01, 5.0000e+01, 3.3100e-09],\n",
    "        [7.0000e+01, 7.0000e+01, 3.3250e-09],\n",
    "        [1.0400e+02, 7.0000e+01, 3.3250e-09],\n",
    "        [1.0000e+00, 7.0000e+01, 3.3250e-09],\n",
    "        [9.6000e+01, 6.9000e+01, 3.3250e-09],\n",
    "        [1.0200e+02, 2.8000e+01, 3.3350e-09],\n",
    "        [1.0900e+02, 4.9000e+01, 3.3400e-09],\n",
    "        [9.8000e+01, 4.6000e+01, 3.6700e-09],\n",
    "        [3.7000e+01, 7.6000e+01, 3.6750e-09],\n",
    "        [7.4000e+01, 5.1000e+01, 3.8800e-09],\n",
    "        [1.1100e+02, 2.4000e+01, 4.1100e-09],\n",
    "        [1.0000e+02, 6.2000e+01, 4.3400e-09],\n",
    "        [6.4000e+01, 0.0000e+00, 4.3500e-09],\n",
    "        [9.4000e+01, 7.0000e+00, 4.7700e-09],\n",
    "        [7.3000e+01, 5.6000e+01, 4.7800e-09],\n",
    "        [5.5000e+01, 5.7000e+01, 4.7850e-09],\n",
    "        [1.0900e+02, 4.0000e+00, 4.7900e-09],\n",
    "        [1.0400e+02, 7.5000e+01, 4.8150e-09]])\n",
    "pos_new = torch.tensor([[1.0400e+02, 7.5000e+01, 4.8150e-09]])\n",
    "distance = torch.cdist(pos_all, pos_new)\n",
    "print(distance.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False,  True, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False,  True]])\n"
     ]
    }
   ],
   "source": [
    "connected_node_mask = torch.cdist(pos_all, pos_new) <= 3.0\n",
    "print(connected_node_mask.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 39.2428,  35.2704,  59.0593,  34.6699,  31.3688,  77.9872,  90.6532,\n",
      "          56.1249,  36.4143,  49.1121,  49.1935,   8.4853,  31.3688,  31.4960,\n",
      "          38.1576,  84.3564,  69.6850,  76.3675,  15.0997,  20.8327,  74.1485,\n",
      "          38.9102,   7.7460,   4.0000,  65.0846,  62.1128,  67.1714,  22.5832,\n",
      "           3.1623,   5.8310,  43.1741,   9.5917,  95.2260,  62.8649,  27.8568,\n",
      "          22.1811,  20.1990,  39.9249,  34.1174,  34.4674,   6.0000, 103.1504,\n",
      "          10.2956,  47.1593,  26.5707,  29.7321,  67.0522,  38.4968,  51.5364,\n",
      "          14.0000,  85.0412,  68.8041,  36.4417,  52.2877,  71.2180,   3.7417]])\n"
     ]
    }
   ],
   "source": [
    "tmp = torch.tensor([[ 39.2428],\n",
    "        [ 35.2704],\n",
    "        [ 59.0593],\n",
    "        [ 34.6699],\n",
    "        [ 31.3688],\n",
    "        [ 77.9872],\n",
    "        [ 90.6532],\n",
    "        [ 56.1249],\n",
    "        [ 36.4143],\n",
    "        [ 49.1121],\n",
    "        [ 49.1935],\n",
    "        [  8.4853],\n",
    "        [ 31.3688],\n",
    "        [ 31.4960],\n",
    "        [ 38.1576],\n",
    "        [ 84.3564],\n",
    "        [ 69.6850],\n",
    "        [ 76.3675],\n",
    "        [ 15.0997],\n",
    "        [ 20.8327],\n",
    "        [ 74.1485],\n",
    "        [ 38.9102],\n",
    "        [  7.7460],\n",
    "        [  4.0000],\n",
    "        [ 65.0846],\n",
    "        [ 62.1128],\n",
    "        [ 67.1714],\n",
    "        [ 22.5832],\n",
    "        [  3.1623],\n",
    "        [  5.8310],\n",
    "        [ 43.1741],\n",
    "        [  9.5917],\n",
    "        [ 95.2260],\n",
    "        [ 62.8649],\n",
    "        [ 27.8568],\n",
    "        [ 22.1811],\n",
    "        [ 20.1990],\n",
    "        [ 39.9249],\n",
    "        [ 34.1174],\n",
    "        [ 34.4674],\n",
    "        [  6.0000],\n",
    "        [103.1504],\n",
    "        [ 10.2956],\n",
    "        [ 47.1593],\n",
    "        [ 26.5707],\n",
    "        [ 29.7321],\n",
    "        [ 67.0522],\n",
    "        [ 38.4968],\n",
    "        [ 51.5364],\n",
    "        [ 14.0000],\n",
    "        [ 85.0412],\n",
    "        [ 68.8041],\n",
    "        [ 36.4417],\n",
    "        [ 52.2877],\n",
    "        [ 71.2180],\n",
    "        [  3.7417]])\n",
    "print(tmp.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 39.1152,  35.1283,  59.0085,  34.5254,  31.2570,  77.9295,  90.6090,\n",
      "         56.0357,  36.3456,  49.0102,  49.0918,   8.0623,  31.2570,  31.4006,\n",
      "         38.0132,  84.3090,  69.6348,  76.3217,  14.7648,  20.5913,  74.1080,\n",
      "         38.8330,   7.0711,   3.1623,  65.0000,  62.0725,  67.1193,  22.4722,\n",
      "          2.2361,   5.3852,  43.0465,   9.0554,  95.1893,  62.8172,  27.6586,\n",
      "         21.9317,  20.0250,  39.8497,  33.9706,  34.3657,   5.0000, 103.1213,\n",
      "         10.0000,  47.0425,  26.4764,  29.6142,  67.0075,  38.4187,  51.4781,\n",
      "         13.6015,  85.0000,  68.7314,  36.3593,  52.2015,  71.1758,   0.0000],\n",
      "       device='cuda:0')\n",
      "tensor([ 39.2428,  35.2704,  59.0593,  34.6699,  31.3688,  77.9872,  90.6532,\n",
      "         56.1249,  36.4143,  49.1121,  49.1935,   8.4853,  31.3688,  31.4960,\n",
      "         38.1576,  84.3564,  69.6850,  76.3675,  15.0997,  20.8327,  74.1485,\n",
      "         38.9102,   7.7460,   4.0000,  65.0846,  62.1128,  67.1714,  22.5832,\n",
      "          3.1623,   5.8310,  43.1741,   9.5917,  95.2260,  62.8649,  27.8568,\n",
      "         22.1811,  20.1990,  39.9249,  34.1174,  34.4674,   6.0000, 103.1504,\n",
      "         10.2956,  47.1593,  26.5707,  29.7321,  67.0522,  38.4968,  51.5364,\n",
      "         14.0000,  85.0412,  68.8041,  36.4417,  52.2877,  71.2180,   3.7417],\n",
      "       device='cuda:0')\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda:0'\n",
    "test_pos_all = torch.tensor([[6.5000e+01, 7.8000e+01, 0.0000e+00],\n",
    "        [6.9000e+01, 7.8000e+01, 0.0000e+00],\n",
    "        [4.5000e+01, 7.6000e+01, 0.0000e+00],\n",
    "        [7.0000e+01, 6.9000e+01, 5.0000e-12],\n",
    "        [1.0000e+02, 4.4000e+01, 1.0000e-11],\n",
    "        [2.7000e+01, 6.3000e+01, 1.4500e-10],\n",
    "        [1.5000e+01, 5.8000e+01, 1.7500e-10],\n",
    "        [1.0600e+02, 1.9000e+01, 1.9000e-10],\n",
    "        [1.0900e+02, 3.9000e+01, 6.6000e-10],\n",
    "        [1.0300e+02, 2.6000e+01, 7.0500e-10],\n",
    "        [1.0100e+02, 2.6000e+01, 7.0500e-10],\n",
    "        [1.0000e+02, 6.8000e+01, 1.0400e-09],\n",
    "        [1.0000e+02, 4.4000e+01, 1.0700e-09],\n",
    "        [1.0900e+02, 4.4000e+01, 1.0750e-09],\n",
    "        [8.2000e+01, 4.4000e+01, 1.0750e-09],\n",
    "        [2.6000e+01, 4.3000e+01, 1.0750e-09],\n",
    "        [4.7000e+01, 3.5000e+01, 1.0800e-09],\n",
    "        [3.9000e+01, 3.5000e+01, 1.0800e-09],\n",
    "        [9.7000e+01, 6.2000e+01, 1.0850e-09],\n",
    "        [9.4000e+01, 5.7000e+01, 1.4150e-09],\n",
    "        [1.0800e+02, 1.0000e+00, 1.7300e-09],\n",
    "        [9.6000e+01, 3.7000e+01, 1.8750e-09],\n",
    "        [9.9000e+01, 7.0000e+01, 1.9500e-09],\n",
    "        [1.0300e+02, 7.2000e+01, 2.0000e-09],\n",
    "        [1.0400e+02, 1.0000e+01, 2.0150e-09],\n",
    "        [1.0700e+02, 1.3000e+01, 2.0950e-09],\n",
    "        [1.0800e+02, 8.0000e+00, 2.1050e-09],\n",
    "        [8.5000e+01, 6.3000e+01, 2.1150e-09],\n",
    "        [1.0300e+02, 7.3000e+01, 2.3800e-09],\n",
    "        [9.9000e+01, 7.3000e+01, 2.3800e-09],\n",
    "        [1.0200e+02, 3.2000e+01, 2.4000e-09],\n",
    "        [9.5000e+01, 7.4000e+01, 2.4050e-09],\n",
    "        [1.0000e+01, 6.0000e+01, 2.5500e-09],\n",
    "        [4.3000e+01, 6.0000e+01, 2.5500e-09],\n",
    "        [9.8000e+01, 4.8000e+01, 2.5550e-09],\n",
    "        [8.4000e+01, 6.6000e+01, 2.5650e-09],\n",
    "        [8.4000e+01, 7.6000e+01, 2.5800e-09],\n",
    "        [9.2000e+01, 3.7000e+01, 2.5850e-09],\n",
    "        [8.1000e+01, 5.0000e+01, 3.3100e-09],\n",
    "        [7.0000e+01, 7.0000e+01, 3.3250e-09],\n",
    "        [1.0400e+02, 7.0000e+01, 3.3250e-09],\n",
    "        [1.0000e+00, 7.0000e+01, 3.3250e-09],\n",
    "        [9.6000e+01, 6.9000e+01, 3.3250e-09],\n",
    "        [1.0200e+02, 2.8000e+01, 3.3350e-09],\n",
    "        [1.0900e+02, 4.9000e+01, 3.3400e-09],\n",
    "        [9.8000e+01, 4.6000e+01, 3.6700e-09],\n",
    "        [3.7000e+01, 7.6000e+01, 3.6750e-09],\n",
    "        [7.4000e+01, 5.1000e+01, 3.8800e-09],\n",
    "        [1.1100e+02, 2.4000e+01, 4.1100e-09],\n",
    "        [1.0000e+02, 6.2000e+01, 4.3400e-09],\n",
    "        [6.4000e+01, 0.0000e+00, 4.3500e-09],\n",
    "        [9.4000e+01, 7.0000e+00, 4.7700e-09],\n",
    "        [7.3000e+01, 5.6000e+01, 4.7800e-09],\n",
    "        [5.5000e+01, 5.7000e+01, 4.7850e-09],\n",
    "        [1.0900e+02, 4.0000e+00, 4.7900e-09],\n",
    "        [1.0400e+02, 7.5000e+01, 4.8150e-09]], device=device)\n",
    "test_pos_new = torch.tensor([[1.0400e+02, 7.5000e+01, 4.8150e-09]], device=device)\n",
    "\n",
    "\n",
    "pos_diff_sqrt = (test_pos_all-test_pos_new).pow(2).sum(1).sqrt()\n",
    "print(pos_diff_sqrt)\n",
    "\n",
    "test_node_dis = torch.cdist(test_pos_all, test_pos_new, p=2).T.squeeze()\n",
    "print(test_node_dis)\n",
    "#! cuda:0 is wrong! cpu is right!!!\n",
    "\n",
    "# pdist = torch.nn.PairwiseDistance(p=2)\n",
    "# test_node_dis = pdist(test_pos_all, test_pos_new)\n",
    "# print(test_node_dis)\n",
    "\n",
    "\n",
    "print(torch.allclose(test_node_dis, pos_diff_sqrt, atol=1e-5))\n",
    "\n",
    "\n",
    "# from_code_node_dis = torch.tensor([[ 39.2428],\n",
    "#         [ 35.2704],\n",
    "#         [ 59.0593],\n",
    "#         [ 34.6699],\n",
    "#         [ 31.3688],\n",
    "#         [ 77.9872],\n",
    "#         [ 90.6532],\n",
    "#         [ 56.1249],\n",
    "#         [ 36.4143],\n",
    "#         [ 49.1121],\n",
    "#         [ 49.1935],\n",
    "#         [  8.4853],\n",
    "#         [ 31.3688],\n",
    "#         [ 31.4960],\n",
    "#         [ 38.1576],\n",
    "#         [ 84.3564],\n",
    "#         [ 69.6850],\n",
    "#         [ 76.3675],\n",
    "#         [ 15.0997],\n",
    "#         [ 20.8327],\n",
    "#         [ 74.1485],\n",
    "#         [ 38.9102],\n",
    "#         [  7.7460],\n",
    "#         [  4.0000],\n",
    "#         [ 65.0846],\n",
    "#         [ 62.1128],\n",
    "#         [ 67.1714],\n",
    "#         [ 22.5832],\n",
    "#         [  3.1623],\n",
    "#         [  5.8310],\n",
    "#         [ 43.1741],\n",
    "#         [  9.5917],\n",
    "#         [ 95.2260],\n",
    "#         [ 62.8649],\n",
    "#         [ 27.8568],\n",
    "#         [ 22.1811],\n",
    "#         [ 20.1990],\n",
    "#         [ 39.9249],\n",
    "#         [ 34.1174],\n",
    "#         [ 34.4674],\n",
    "#         [  6.0000],\n",
    "#         [103.1504],\n",
    "#         [ 10.2956],\n",
    "#         [ 47.1593],\n",
    "#         [ 26.5707],\n",
    "#         [ 29.7321],\n",
    "#         [ 67.0522],\n",
    "#         [ 38.4968],\n",
    "#         [ 51.5364],\n",
    "#         [ 14.0000],\n",
    "#         [ 85.0412],\n",
    "#         [ 68.8041],\n",
    "#         [ 36.4417],\n",
    "#         [ 52.2877],\n",
    "#         [ 71.2180],\n",
    "#         [  3.7417]])\n",
    "# # print(from_code_node_dis.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([28])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "\n",
    "diff = tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 0, 1],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "\n",
    "a = torch.sum(diff, dim=1)\n",
    "b = torch.nonzero(a)[:, 0]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sync, graph:\n",
      "after conv1:\n",
      "tensor([[0.0754, 0.0169],\n",
      "        [0.1102, 0.0223],\n",
      "        [0.0604, 0.0101],\n",
      "        [0.0797, 0.0150],\n",
      "        [0.1059, 0.0210],\n",
      "        [0.1442, 0.0366],\n",
      "        [0.0834, 0.0146],\n",
      "        [0.1061, 0.0217]], grad_fn=<DivBackward0>)\n",
      "\n",
      "after elu1:\n",
      "tensor([[0.0754, 0.0169],\n",
      "        [0.1102, 0.0223],\n",
      "        [0.0604, 0.0101],\n",
      "        [0.0797, 0.0150],\n",
      "        [0.1059, 0.0210],\n",
      "        [0.1442, 0.0366],\n",
      "        [0.0834, 0.0146],\n",
      "        [0.1061, 0.0217]], grad_fn=<EluBackward0>)\n",
      "\n",
      "after conv2:\n",
      "tensor([[-0.0053, -0.0028, -0.0182,  0.0049],\n",
      "        [-0.0037, -0.0015, -0.0118,  0.0021],\n",
      "        [-0.0054, -0.0018, -0.0167,  0.0023],\n",
      "        [-0.0062, -0.0020, -0.0142,  0.0010],\n",
      "        [-0.0050, -0.0020, -0.0152,  0.0027],\n",
      "        [-0.0043, -0.0019, -0.0135,  0.0035],\n",
      "        [-0.0081, -0.0035, -0.0229,  0.0027],\n",
      "        [-0.0058, -0.0022, -0.0161,  0.0025]], grad_fn=<DivBackward0>)\n",
      "\n",
      "sync graph.num_nodes = 8\n",
      "graph:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import aegnn\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, SplineConv, BatchNorm\n",
    "from torch.nn.functional import elu\n",
    "import pytorch_lightning as pl\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "pl.seed_everything(12345)\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# def network\n",
    "class net(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = SplineConv(1, 2, dim=2, kernel_size=2, bias=False, root_weight=False)\n",
    "        self.conv2 = SplineConv(2, 4, dim=2, kernel_size=2, bias=False, root_weight=False)\n",
    "        # self.conv1 = GCNConv(1, 2)\n",
    "        # self.conv2 = GCNConv(2, 4)\n",
    "        # self.norm1 = BatchNorm(in_channels=2)\n",
    "        self.act = elu\n",
    "\n",
    "    def forward(self, data):\n",
    "        data.x = self.conv1(data.x, data.edge_index, data.edge_attr)\n",
    "        print(f'after conv1:\\n{data.x}\\n')\n",
    "        data.x = self.act(data.x)\n",
    "        print(f'after elu1:\\n{data.x}\\n')\n",
    "        data.x = self.conv2(data.x, data.edge_index, data.edge_attr)\n",
    "        print(f'after conv2:\\n{data.x}\\n')\n",
    "        return data.x\n",
    "\n",
    "module = net()\n",
    "\n",
    "\n",
    "# def graph for sync\n",
    "attr_func = torch_geometric.transforms.Cartesian(cat=False, max_value=10.0)\n",
    "graph = Data(\n",
    "    x = torch.tensor([1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], device=device).view(-1,1),\n",
    "    pos = torch.tensor([[2,3],[1,2],[1,1],[3,1],[3,2],[5,3],[5,2],[4,2]], device=device),\n",
    "    edge_index = torch.tensor([[0,1,1,2,2,3,3,4,4,0,4,7,7,6,6,5], [1,0,2,1,3,2,4,3,0,4,7,4,6,7,5,6]], device=device, dtype=torch.long))\n",
    "graph = attr_func(graph)\n",
    "print('sync, graph:')\n",
    "output = module.forward(graph)\n",
    "print(f'sync graph.num_nodes = {graph.num_nodes}')\n",
    "print('graph:')\n",
    "g_nx = to_networkx(graph, to_undirected=True)\n",
    "nx.draw_networkx(g_nx, with_labels=True, pos=graph.pos.tolist(), node_size=0, node_shape='.',  font_size=8, font_color='k',  edge_color='b', width=1)\n",
    "\n",
    "# def graph_init, node_x for AEGNN\n",
    "x = torch.tensor([1.0, 0.0, 1.0, 1.0, 1.0], device=device).view(-1,1)\n",
    "edge_index = torch.tensor([[0,1,1,2,2,3,3,4,4,0],\n",
    "                           [1,0,2,1,3,2,4,3,0,4]], device=device, dtype=torch.long)\n",
    "pos = torch.tensor([[2,3],[1,2],[1,1],[3,1],[3,2]], device=device)\n",
    "\n",
    "graph_init = Data(x=x, pos=pos, edge_index=edge_index)\n",
    "graph_init = attr_func(graph_init)\n",
    "\n",
    "node_1 = Data(x=torch.tensor([[1.0]], device=device), pos=torch.tensor([[5,3]], device=device))\n",
    "node_2 = Data(x=torch.tensor([[1.0]], device=device), pos=torch.tensor([[5,2]], device=device))\n",
    "node_3 = Data(x=torch.tensor([[1.0]], device=device), pos=torch.tensor([[4,2]], device=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async, init:\n",
      "after conv1:\n",
      "tensor([[0.0754, 0.0169],\n",
      "        [0.1102, 0.0223],\n",
      "        [0.0604, 0.0101],\n",
      "        [0.0797, 0.0150],\n",
      "        [0.1021, 0.0210]], grad_fn=<DivBackward0>)\n",
      "\n",
      "after elu1:\n",
      "tensor([[0.0754, 0.0169],\n",
      "        [0.1102, 0.0223],\n",
      "        [0.0604, 0.0101],\n",
      "        [0.0797, 0.0150],\n",
      "        [0.1021, 0.0210]], grad_fn=<EluBackward0>)\n",
      "\n",
      "after conv2:\n",
      "tensor([[-0.0051, -0.0028, -0.0180,  0.0049],\n",
      "        [-0.0037, -0.0015, -0.0118,  0.0021],\n",
      "        [-0.0054, -0.0018, -0.0167,  0.0023],\n",
      "        [-0.0060, -0.0020, -0.0140,  0.0010],\n",
      "        [-0.0049, -0.0020, -0.0135,  0.0021]], grad_fn=<DivBackward0>)\n",
      "\n",
      "async, add node 1:\n",
      "after conv1:\n",
      "tensor([[0.0754, 0.0169],\n",
      "        [0.1102, 0.0223],\n",
      "        [0.0604, 0.0101],\n",
      "        [0.0797, 0.0150],\n",
      "        [0.1021, 0.0210],\n",
      "        [0.0000, 0.0000]], grad_fn=<CatBackward0>)\n",
      "\n",
      "after elu1:\n",
      "tensor([[0.0754, 0.0169],\n",
      "        [0.1102, 0.0223],\n",
      "        [0.0604, 0.0101],\n",
      "        [0.0797, 0.0150],\n",
      "        [0.1021, 0.0210],\n",
      "        [0.0000, 0.0000]], grad_fn=<EluBackward0>)\n",
      "\n",
      "after conv2:\n",
      "tensor([[-0.0051, -0.0028, -0.0180,  0.0049],\n",
      "        [-0.0037, -0.0015, -0.0118,  0.0021],\n",
      "        [-0.0054, -0.0018, -0.0167,  0.0023],\n",
      "        [-0.0060, -0.0020, -0.0140,  0.0010],\n",
      "        [-0.0049, -0.0020, -0.0135,  0.0021],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000]], grad_fn=<CatBackward0>)\n",
      "\n",
      "async, add node 2:\n",
      "after conv1:\n",
      "tensor([[0.0754, 0.0169],\n",
      "        [0.1102, 0.0223],\n",
      "        [0.0604, 0.0101],\n",
      "        [0.0797, 0.0150],\n",
      "        [0.1021, 0.0210],\n",
      "        [0.1442, 0.0366],\n",
      "        [0.0680, 0.0068]], grad_fn=<IndexPutBackward0>)\n",
      "\n",
      "after elu1:\n",
      "tensor([[0.0754, 0.0169],\n",
      "        [0.1102, 0.0223],\n",
      "        [0.0604, 0.0101],\n",
      "        [0.0797, 0.0150],\n",
      "        [0.1021, 0.0210],\n",
      "        [0.1442, 0.0366],\n",
      "        [0.0680, 0.0068]], grad_fn=<EluBackward0>)\n",
      "\n",
      "after conv2:\n",
      "tensor([[-0.0051, -0.0028, -0.0180,  0.0049],\n",
      "        [-0.0037, -0.0015, -0.0118,  0.0021],\n",
      "        [-0.0054, -0.0018, -0.0167,  0.0023],\n",
      "        [-0.0060, -0.0020, -0.0140,  0.0010],\n",
      "        [-0.0049, -0.0020, -0.0135,  0.0021],\n",
      "        [-0.0042, -0.0010, -0.0098,  0.0022],\n",
      "        [-0.0089, -0.0040, -0.0274,  0.0029]], grad_fn=<IndexPutBackward0>)\n",
      "\n",
      "async, add node 3:\n",
      "after conv1:\n",
      "tensor([[0.0754, 0.0169],\n",
      "        [0.1102, 0.0223],\n",
      "        [0.0604, 0.0101],\n",
      "        [0.0797, 0.0150],\n",
      "        [0.1059, 0.0210],\n",
      "        [0.1442, 0.0366],\n",
      "        [0.0834, 0.0146],\n",
      "        [0.1061, 0.0217]], grad_fn=<IndexPutBackward0>)\n",
      "\n",
      "after elu1:\n",
      "tensor([[0.0754, 0.0169],\n",
      "        [0.1102, 0.0223],\n",
      "        [0.0604, 0.0101],\n",
      "        [0.0797, 0.0150],\n",
      "        [0.1059, 0.0210],\n",
      "        [0.1442, 0.0366],\n",
      "        [0.0834, 0.0146],\n",
      "        [0.1061, 0.0217]], grad_fn=<EluBackward0>)\n",
      "\n",
      "after conv2:\n",
      "tensor([[-0.0053, -0.0028, -0.0182,  0.0049],\n",
      "        [-0.0037, -0.0015, -0.0118,  0.0021],\n",
      "        [-0.0054, -0.0018, -0.0167,  0.0023],\n",
      "        [-0.0062, -0.0020, -0.0142,  0.0010],\n",
      "        [-0.0050, -0.0020, -0.0152,  0.0027],\n",
      "        [-0.0043, -0.0019, -0.0135,  0.0035],\n",
      "        [-0.0081, -0.0035, -0.0229,  0.0027],\n",
      "        [-0.0058, -0.0022, -0.0161,  0.0025]], grad_fn=<IndexPutBackward0>)\n",
      "\n",
      "conv2.asy_graph.num_nodes: \n",
      "8\n",
      "graph:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# async model\n",
    "module = aegnn.asyncronous.make_model_asynchronous(module, 1.0, (10,10), attr_func)\n",
    "print('async, init:')\n",
    "module.forward(graph_init)\n",
    "print('async, add node 1:')\n",
    "module.forward(node_1)\n",
    "print('async, add node 2:')\n",
    "module.forward(node_2)\n",
    "print('async, add node 3:')\n",
    "module.forward(node_3)\n",
    "\n",
    "asy_graph = module.conv2.asy_graph\n",
    "print(f'conv2.asy_graph.num_nodes: \\n{asy_graph.num_nodes}')\n",
    "print('graph:')\n",
    "asy_graph_nx = to_networkx(asy_graph, to_undirected=True)\n",
    "nx.draw_networkx(asy_graph_nx, with_labels=True, pos=asy_graph.pos.tolist(), node_size=0, node_shape='.',  font_size=8, font_color='k',  edge_color='b', width=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('aegnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9439450e489ce535473a2847795b2c81cbeeccb2f39d71287859ebd0392d6b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
