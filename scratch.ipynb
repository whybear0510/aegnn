{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import aegnn\n",
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning.metrics.functional as pl_metrics\n",
    "\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.utils import subgraph\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from torch.nn import Linear\n",
    "from torch.nn.functional import elu\n",
    "from torch_geometric.nn.conv import SplineConv\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "from torch_geometric.transforms import Cartesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 4000 samples, wrt their own events/nodes:\n",
    "```\n",
    "acc_Mflop_per_ev = 134.74224981457488, avg_Mflop_per_ev = 0.03368556245364372\n",
    "dense: std acc_Mflop_per_ev = 1170.527851753108, std avg_Mflop_per_ev = 0.29263196293827703\n",
    "```\n",
    "\n",
    "For 100 samples, wrt their own events/nodes:\n",
    "```\n",
    "acc_Mflop_per_ev = 3.414027443525552, avg_Mflop_per_ev = 0.034140274435255524\n",
    "dense: std acc_Mflop_per_ev = 28.595094207357473, std avg_Mflop_per_ev = 0.28595094207357474\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrected formula for calculating FLOPS (according to updated paper), leading to:\n",
    "\n",
    "For 4000 samples, wrt their own events/nodes:\n",
    "```\n",
    "acc_Mflop_per_ev = 240.93893365788432, avg_Mflop_per_ev = 0.06023473341447108\n",
    "dense: std acc_Mflop_per_ev = 2096.876591199398, std avg_Mflop_per_ev = 0.5242191477998495\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = aegnn.datasets.NCars(batch_size=1, shuffle=False)\n",
    "data_module.setup()\n",
    "dm = data_module.train_dataset\n",
    "\n",
    "# cnt = 0\n",
    "# for i, dms in enumerate(dm):\n",
    "#     cnt += 1\n",
    "#     print(i)\n",
    "\n",
    "# print(cnt)\n",
    "\n",
    "# num_trials=100\n",
    "# nodes=[]\n",
    "# nodes_cnt=0\n",
    "# for index in tqdm(range(num_trials)):\n",
    "#     sample = dm[index % len(dm)]\n",
    "#     nodes.append(sample.num_nodes)\n",
    "#     nodes_cnt += sample.num_nodes\n",
    "# print(nodes_cnt)\n",
    "\n",
    "print(dm.dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if add async code into train.py\n",
    "```\n",
    "File \"scripts/train.py\", line 97, in <module>\n",
    "    main(arguments)\n",
    "  File \"scripts/train.py\", line 91, in main\n",
    "    trainer.fit(model, datamodule=dm)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 552, in fit\n",
    "    self._run(model)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 922, in _run\n",
    "    self._dispatch()\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 990, in _dispatch\n",
    "    self.accelerator.start_training(self)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 92, in start_training\n",
    "    self.training_type_plugin.start_training(trainer)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 161, in \n",
    "start_training\n",
    "    self._results = trainer.run_stage()\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1000, in run_stage\n",
    "    return self._run_train()\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1049, in _run_train\n",
    "    self.fit_loop.run()\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\n",
    "    self.advance(*args, **kwargs)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 200, in advance\n",
    "    epoch_output = self.epoch_loop.run(train_dataloader)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 118, in run\n",
    "    output = self.on_run_end()\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 235, in on_run_end\n",
    "    self._on_train_epoch_end_hook(processed_outputs)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 276, in _on_train_epoch_end_hook\n",
    "    trainer_hook(processed_epoch_output)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/callback_hook.py\", line 109, in on_train_epoch_end\n",
    "    callback.on_train_epoch_end(self, self.lightning_module)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 310, in on_train_epoch_end  \n",
    "    self.save_checkpoint(trainer)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 386, in save_checkpoint\n",
    "    self._save_none_monitor_checkpoint(trainer, monitor_candidates)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 742, in _save_none_monitor_checkpoint\n",
    "    self._save_model(trainer, filepath)\n",
    "  File \"/users/yyang22/thesis/aegnn_project/aegnn/aegnn/utils/callbacks/checkpoint_full_model.py\", line 14, in _save_model\n",
    "    torch.save(trainer.model, filepath)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/torch/serialization.py\", line 380, in save\n",
    "    _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/torch/serialization.py\", line 589, in _save\n",
    "    pickler.dump(obj)\n",
    "AttributeError: Can't pickle local object 'make_model_asynchronous.<locals>.async_forward'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if I use wandb:\n",
    "```\n",
    "File \"scripts/train.py\", line 97, in <module>\n",
    "    main(arguments)\n",
    "  File \"scripts/train.py\", line 91, in main\n",
    "    trainer.fit(model, datamodule=dm)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 552, in fit\n",
    "    self._run(model)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 922, in _run\n",
    "    self._dispatch()\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 990, in _dispatch\n",
    "    self.accelerator.start_training(self)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 92, in start_training\n",
    "    self.training_type_plugin.start_training(trainer)  \n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 161, in \n",
    "start_training\n",
    "    self._results = trainer.run_stage()\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1000, in run_stage\n",
    "    return self._run_train()\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1049, in _run_train\n",
    "    self.fit_loop.run()\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\n",
    "    self.advance(*args, **kwargs)\n",
    "File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 200, in advance\n",
    "    epoch_output = self.epoch_loop.run(train_dataloader)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 118, in run\n",
    "    output = self.on_run_end()\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 235, in on_run_end\n",
    "    self._on_train_epoch_end_hook(processed_outputs)   \n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 276, in _on_train_e\n",
    "poch_end_hook\n",
    "    trainer_hook(processed_epoch_output)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/trainer/callback_hook.py\", line 109, in on_train_epoch_end\n",
    "    callback.on_train_epoch_end(self, self.lightning_module)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 310, in on_train_epoch_end\n",
    "    self.save_checkpoint(trainer)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 386, in save_checkpoint\n",
    "    self._save_none_monitor_checkpoint(trainer, monitor_candidates)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 742, in _save_none_monitor_checkpoint\n",
    "    self._save_model(trainer, filepath)\n",
    "  File \"/users/yyang22/thesis/aegnn_project/aegnn/aegnn/utils/callbacks/checkpoint_full_model.py\", line 14, in _save_model\n",
    "    torch.save(trainer.model, filepath)\n",
    "File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/torch/serialization.py\", line 380, in save\n",
    "    _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n",
    "  File \"/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/torch/serialization.py\", line 589, in _save\n",
    "    pickler.dump(obj)\n",
    "AttributeError: Can't pickle local object 'Settings._validator_factory.<locals>.helper'\n",
    "```\n",
    "Problems seem to be at logger function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb import sdk as wandb_sdk\n",
    "\n",
    "wandb.init(project=\"aegnn\", entity=\"yyfteam\")\n",
    "# log_settings = wandb.Settings(start_method=\"thread\")\n",
    "log_settings = wandb.Settings(start_method=\"fork\")\n",
    "wandb_sdk.Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aegnn\n",
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "edge_attr = torch_geometric.transforms.Cartesian(cat=False, max_value=10.0)\n",
    "\n",
    "print(type(edge_attr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "initial_lr = 0.5\n",
    "\n",
    "\n",
    "\n",
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "net_1 = model()\n",
    "\n",
    "def LRPolicy(epoch):\n",
    "    if epoch < 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "optimizer_1 = torch.optim.Adam(net_1.parameters(), lr = initial_lr)\n",
    "scheduler_1 = LambdaLR(optimizer_1, lr_lambda=LRPolicy)\n",
    "\n",
    "print(\"init lr\", optimizer_1.defaults['lr'])\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    # train\n",
    "    optimizer_1.zero_grad()\n",
    "    optimizer_1.step()\n",
    "    print(\"lr of %dth epoch: %f\" % (epoch, optimizer_1.param_groups[0]['lr']))\n",
    "    scheduler_1.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "path='../aegnn_results/training_results/latest'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "else:\n",
    "    # clean\n",
    "    try:\n",
    "        shutil.rmtree(path)\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s : %s\" % (path, e.strerror))\n",
    "    \n",
    "    # rebuild\n",
    "    os.makedirs(path)\n",
    "\n",
    "src_model = sorted(glob.glob(r'/space/yyang22/datasets/data/scratch/checkpoints/ncars/recognition/*/*.pt'), key=os.path.getctime)[-1]\n",
    "dst_model = os.path.join(path,'latest_model.pt')\n",
    "\n",
    "src_log = sorted(glob.glob(r'/space/yyang22/datasets/data/scratch/debug/*'), key=os.path.getctime)[-1]\n",
    "dst_log = os.path.join(path,'latest.log')\n",
    "\n",
    "print(src_model,dst_model,src_log,dst_log)\n",
    "try:\n",
    "    shutil.copy2(src_model, dst_model)\n",
    "except IOError as e:\n",
    "    print(\"Unable to copy file. %s\" % e)\n",
    "except:\n",
    "    print(\"Unexpected error:\", sys.exc_info())\n",
    "\n",
    "try:\n",
    "    shutil.copy2(src_log, dst_log)\n",
    "except IOError as e:\n",
    "    print(\"Unable to copy file. %s\" % e)\n",
    "except:\n",
    "    print(\"Unexpected error:\", sys.exc_info())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cmd = 'python3 ../../test_bkgnd.py'\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3, 1], edge_index=[2, 4], edge_attr=[1, 3], pos=[3, 3])\n",
      "Data(x=[2, 1], edge_index=[2, 2], edge_attr=[1, 3], pos=[2, 3])\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.]])\n",
      "tensor([[0, 1, 1, 2, 3, 4],\n",
      "        [1, 0, 2, 1, 4, 3]])\n",
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 3 required positional arguments: 'dataset', 'input_shape', and 'num_outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m g \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     37\u001b[0m \u001b[39m# net1 = GraphRes(2,4)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m# model_file = '/users/yyang22/thesis/aegnn_project/aegnn_results/training_results/checkpoints/ncars/recognition/20221125084923/epoch=99-step=20299.pt'\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m# net1 = torch.load(model_file).to(torch.device('cuda'))\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m net1 \u001b[39m=\u001b[39m aegnn\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mnetworks\u001b[39m.\u001b[39;49mgraph_res\u001b[39m.\u001b[39;49mGraphRes()\n\u001b[1;32m     42\u001b[0m out1 \u001b[39m=\u001b[39m net1(g1)\n\u001b[1;32m     43\u001b[0m out2 \u001b[39m=\u001b[39m net1(g2)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 3 required positional arguments: 'dataset', 'input_shape', and 'num_outputs'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "import aegnn\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "\n",
    "class GraphRes(torch.nn.Module):\n",
    "    def __init__(self, cin, cout):\n",
    "        super(GraphRes, self).__init__()\n",
    "        self.conv1 = GCNConv(cin, cout)\n",
    "\n",
    "    def forward(self, data: torch_geometric.data.Batch) -> torch.Tensor:\n",
    "        data.x = self.conv1(data.x, data.edge_index)\n",
    "        return data\n",
    "\n",
    "edge_attr = torch.tensor([[0.0,0.0,0.0]])\n",
    "\n",
    "edge_index1 = torch.tensor([[0,1,1,2],[1,0,2,1]], dtype=torch.long)\n",
    "x1 = torch.tensor([[1.0],[2.0],[3.0]])\n",
    "pos1 = torch.tensor([0.,0.,0., 1.,1.,0., 2.,0.,0.]).view(3,3)\n",
    "g1 = torch_geometric.data.Data(x=x1, edge_index=edge_index1, pos=pos1, edge_attr=edge_attr)\n",
    "print(g1)\n",
    "\n",
    "edge_index2 = torch.tensor([[0,1],[1,0]], dtype=torch.long)\n",
    "x2 = torch.tensor([[4.0],[5.0]])\n",
    "pos2 = torch.tensor([0.,0.,1., 1.,1.,1.]).view(2,3)\n",
    "g2 = torch_geometric.data.Data(x=x2, edge_index=edge_index2, pos=pos2, edge_attr=edge_attr)\n",
    "print(g2)\n",
    "\n",
    "g = torch_geometric.data.Batch.from_data_list([g1,g2])\n",
    "print(g.x)\n",
    "print(g.edge_index)\n",
    "print(g.batch)\n",
    "\n",
    "g1 = g1.to(torch.device('cuda'))\n",
    "g2 = g2.to(torch.device('cuda'))\n",
    "g = g.to(torch.device('cuda'))\n",
    "net1 = GraphRes(1,4)\n",
    "# model_file = '/users/yyang22/thesis/aegnn_project/aegnn_results/training_results/checkpoints/ncars/recognition/20221125084923/epoch=99-step=20299.pt'\n",
    "# net1 = torch.load(model_file).to(torch.device('cuda'))\n",
    "# net1 = aegnn.models.networks.graph_res.GraphRes('ncars',)\n",
    "\n",
    "out1 = net1(g1)\n",
    "out2 = net1(g2)\n",
    "out = net1(g)\n",
    "\n",
    "# for param in net1.parameters():\n",
    "#     print(param)\n",
    "\n",
    "print(out1.x)\n",
    "print(out2.x)\n",
    "print(out.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "``CUDA_VISIBLE_DEVICES=5 wandb agent --count 4 yyfteam/aegnn/yzqyfzg6``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in preprocessing.py, order of calling:\n",
    "\n",
    "``ncars``\n",
    "\n",
    "-> ``event_dm.py: prepare_data(self)`` --> ``_prepare_dataset(self)``\n",
    "\n",
    "--> ``ncaltech101.py: _prepare_dataset(self)`` ---> ``processing()@static (parallel)``\n",
    "\n",
    "---> \n",
    "\n",
    " + ``ncars.py: load()@static``  : from text, read ``[x,y,t,p(0/1)]``\n",
    "  + ``ncars.py: read_label()@static``  : 0=car, 1=background; data.label = name, data.y = value\n",
    " + ``ncars.py: pre_transform(self)`` \n",
    "\n",
    "   -----> \n",
    "   + ``.util.normalization.py: normalize_time()`` : ``t_new = (ts - torch.min(ts)) * beta``, beta: float = 0.5e-5\n",
    "   + ``ncaltech101.py: sub_sampling()@static``\n",
    "\n",
    "    &nbsp;&nbsp; ------>\n",
    "    \n",
    "    &nbsp;&nbsp; + ``from torch_geometric.transforms import FixedPoints``: ``FixedPoints(num=10000, allow_duplicates=False, replace=False)`` : will shuffle and subsample events in a event stream !\n",
    "    \n",
    "   + ``from torch_geometric.nn.pool import radius_graph`` : add edge_index by radius_graph\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12345\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: all CUDA-capable devices are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [87], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m raw_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/space/yyang22/datasets/data/storage/ncars/training/sequence_0/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[39m# raw_file = '/space/yyang22/datasets/data/storage/ncars/training/sequence_1118/'\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[39m# load x, pos\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m event_data \u001b[39m=\u001b[39m load(raw_file)\n\u001b[1;32m     34\u001b[0m \u001b[39m# load label name and label value\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m (label \u001b[39m:=\u001b[39m read_label(raw_file)) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn [87], line 16\u001b[0m, in \u001b[0;36mload\u001b[0;34m(raw_file)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(raw_file: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Data:\n\u001b[1;32m     15\u001b[0m     events_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(raw_file, \u001b[39m\"\u001b[39m\u001b[39mevents.txt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     events \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(np\u001b[39m.\u001b[39;49mloadtxt(events_file))\u001b[39m.\u001b[39;49mfloat()\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m     17\u001b[0m     x, pos \u001b[39m=\u001b[39m events[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:], events[:, :\u001b[39m3\u001b[39m]\n\u001b[1;32m     18\u001b[0m     \u001b[39mreturn\u001b[39;00m Data(x\u001b[39m=\u001b[39mx, pos\u001b[39m=\u001b[39mpos)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: all CUDA-capable devices are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import aegnn\n",
    "import os\n",
    "from typing import Callable, List, Optional, Union\n",
    "\n",
    "# torch.cuda.set_device(5)\n",
    "\n",
    "pl.seed_everything(12345)\n",
    "\n",
    "def load(raw_file: str) -> Data:\n",
    "    events_file = os.path.join(raw_file, \"events.txt\")\n",
    "    events = torch.from_numpy(np.loadtxt(events_file)).float().cuda()\n",
    "    x, pos = events[:, -1:], events[:, :3]\n",
    "    return Data(x=x, pos=pos)\n",
    "\n",
    "def read_label(raw_file: str) -> Optional[Union[str, List[str]]]:\n",
    "    label_file = os.path.join(raw_file, \"is_car.txt\")\n",
    "    with open(label_file, \"r\") as f:\n",
    "        label_txt = f.read().replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "    return \"car\" if label_txt == \"1\" else \"background\"\n",
    "\n",
    "\n",
    "class_dict = {class_id: i for i, class_id in enumerate([\"car\", \"background\"])}  # here, car=0, background=1. I dont know why...\n",
    "raw_file = '/space/yyang22/datasets/data/storage/ncars/training/sequence_0/'\n",
    "# raw_file = '/space/yyang22/datasets/data/storage/ncars/training/sequence_1118/'\n",
    "\n",
    "# load x, pos\n",
    "event_data = load(raw_file)\n",
    "\n",
    "# load label name and label value\n",
    "if (label := read_label(raw_file)) is not None:\n",
    "    event_data.label = label if isinstance(label, list) else [label]\n",
    "    event_data.y = torch.tensor([class_dict[label] for label in event_data.label])\n",
    "\n",
    "print(event_data)\n",
    "print(event_data.x.T)\n",
    "print(event_data.pos)\n",
    "print(event_data.pos[:,-1])\n",
    "print(event_data.label)\n",
    "print(event_data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [83], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[39mreturn\u001b[39;00m data\n\u001b[1;32m     13\u001b[0m \u001b[39m# event_data = sub_sampling(event_data, 10000, True)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m event_data \u001b[39m=\u001b[39m sub_sampling(event_data, \u001b[39m10000\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(event_data)\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(event_data\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mT)\n",
      "Cell \u001b[0;32mIn [83], line 10\u001b[0m, in \u001b[0;36msub_sampling\u001b[0;34m(data, n_samples, sub_sample)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m key, item \u001b[39min\u001b[39;00m data:\n\u001b[1;32m      9\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mis_tensor(item) \u001b[39mand\u001b[39;00m item\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 10\u001b[0m         data[key] \u001b[39m=\u001b[39m item[sample_idx]\n\u001b[1;32m     11\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "from torch_geometric.transforms import FixedPoints\n",
    "def sub_sampling(data: Data, n_samples: int, sub_sample: bool) -> Data:\n",
    "    if sub_sample:\n",
    "        sampler = FixedPoints(num=n_samples, allow_duplicates=False, replace=False)\n",
    "        return sampler(data)\n",
    "    else:\n",
    "        sample_idx = np.arange(n_samples)\n",
    "        for key, item in data:\n",
    "            if torch.is_tensor(item) and item.size(0) != 1:\n",
    "                data[key] = item[sample_idx]\n",
    "        return data\n",
    "\n",
    "# event_data = sub_sampling(event_data, 10000, True)\n",
    "event_data = sub_sampling(event_data, 10000, False)\n",
    "\n",
    "print(event_data)\n",
    "print(event_data.x.T)\n",
    "print(event_data.pos)\n",
    "print(event_data.pos[:,-1])\n",
    "print(event_data.label)\n",
    "print(event_data.y)\n",
    "\n",
    "print(torch.min(event_data.pos[:,-1]))\n",
    "print(torch.max(event_data.pos[:,-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.0000e+00, 1.9000e+01, 1.4963e-07],\n",
      "        [7.0000e+00, 3.6000e+01, 2.1136e-07],\n",
      "        [6.6000e+01, 1.1000e+01, 3.2841e-07],\n",
      "        ...,\n",
      "        [2.5000e+01, 2.7000e+01, 4.7212e-07],\n",
      "        [3.0000e+00, 7.0000e+00, 4.8742e-07],\n",
      "        [2.0000e+00, 1.1000e+01, 2.3369e-07]], device='cuda:1')\n",
      "tensor(0., device='cuda:1')\n",
      "tensor(79., device='cuda:1')\n",
      "tensor(0., device='cuda:1')\n",
      "tensor(42., device='cuda:1')\n",
      "tensor(0., device='cuda:1')\n",
      "tensor(4.9966e-07, device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "event_data.pos[:, 2] = (event_data.pos[:, 2] - torch.min(event_data.pos[:, 2])) * 0.5e-5\n",
    "print(event_data.pos)\n",
    "print(torch.min(event_data.pos[:,0]))\n",
    "print(torch.max(event_data.pos[:,0]))\n",
    "\n",
    "print(torch.min(event_data.pos[:,1]))\n",
    "print(torch.max(event_data.pos[:,1]))\n",
    "\n",
    "print(torch.min(event_data.pos[:,2]))\n",
    "print(torch.max(event_data.pos[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  28,   59,   70,  ..., 4625, 4827, 4907],\n",
      "        [   0,    0,    0,  ..., 6262, 6262, 6262]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn.pool import radius_graph\n",
    "event_data.edge_index = radius_graph(event_data.pos, r=3.0, max_num_neighbors=32)\n",
    "print(event_data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[6263, 1], pos=[6263, 3], file_id='sequence_0', label=[1], y=[1], edge_index=[2, 190656])\n",
      "tensor([[2.3000e+01, 2.6000e+01, 2.4466e-07],\n",
      "        [3.0000e+01, 3.6000e+01, 1.0063e-07],\n",
      "        [2.0000e+01, 1.7000e+01, 4.7357e-07],\n",
      "        ...,\n",
      "        [4.1000e+01, 9.0000e+00, 3.6295e-08],\n",
      "        [1.9000e+01, 1.9000e+01, 3.9599e-07],\n",
      "        [2.0000e+01, 3.0000e+00, 4.7259e-07]], device='cuda:1')\n",
      "['car']\n",
      "tensor([0], device='cuda:1')\n",
      "tensor(0., device='cuda:1')\n",
      "tensor(79., device='cuda:1')\n",
      "tensor(0., device='cuda:1')\n",
      "tensor(42., device='cuda:1')\n",
      "tensor(0., device='cuda:1')\n",
      "tensor(4.9966e-07, device='cuda:1')\n",
      "tensor([[2739, 5759, 6107,  ..., 3643, 4029, 1174],\n",
      "        [   0,    0,    0,  ..., 6262, 6262, 6262]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "processed = '/space/yyang22/datasets/data/storage/ncars/processed/training/sequence_0'\n",
    "data2 = torch.load(processed).to(torch.device('cuda'))\n",
    "print(data2)\n",
    "print(data2.pos)\n",
    "print(data2.label)\n",
    "print(data2.y)\n",
    "print(torch.min(data2.pos[:,0]))\n",
    "print(torch.max(data2.pos[:,0]))\n",
    "\n",
    "print(torch.min(data2.pos[:,1]))\n",
    "print(torch.max(data2.pos[:,1]))\n",
    "\n",
    "print(torch.min(data2.pos[:,2]))\n",
    "print(torch.max(data2.pos[:,2]))\n",
    "print(data2.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 2])\n",
      "\n",
      "tensor([3, 3, 3])\n",
      "tensor([0, 0, 0, 2, 7, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def fixed_voxel_grid(pos: Tensor, full_shape: Tensor, size: Tensor, batch: Tensor = None) -> Tensor:\n",
    "\n",
    "    # device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    # params and check\n",
    "    node_dims = pos.size(1)\n",
    "    num_nodes = pos.size(0)\n",
    "    assert len(full_shape) == node_dims\n",
    "    assert len(size)==node_dims or len(size)==1\n",
    "\n",
    "    # batch is None when a single sample\n",
    "    if batch is None:\n",
    "        batch = torch.zeros(num_nodes, dtype=torch.long)\n",
    "\n",
    "    # counting how many grids in each dimension, upward ceiling\n",
    "    num_grids = torch.squeeze(torch.ceil(torch.div(full_shape, size)))\n",
    "\n",
    "    # according to node's pos, calculating its idx (x,y,z,...) in grids\n",
    "    idx = torch.div(pos, size, rounding_mode='floor')\n",
    "    # batch is natually the batch_size idx; transposition for later matmul\n",
    "    idx = torch.cat([idx, batch.view(-1,1)], dim=1).T\n",
    "\n",
    "    # calculating accumulated indices: for grids with (A,B,C,..) voxels idx, and point (x,y,z,...)\n",
    "    # the accumulated indices are: (1,A,AB,ABC,...)\n",
    "    acc_idx = torch.ones(node_dims+1, device=device)\n",
    "    for i in range(node_dims):\n",
    "        acc_idx[i+1] = acc_idx[i] * num_grids[i]\n",
    "\n",
    "    # final index is x*1 + y*A + z*AB + ...., which equals to a vector times the idx\n",
    "    cluster = (acc_idx @ idx).type(torch.long)\n",
    "\n",
    "    return cluster\n",
    "\n",
    "\n",
    "pos = torch.tensor([0.1,0.1, 0.2,0.2, 0.3,0.3, 0.1,0.9], dtype=torch.float).view(-1,2) \n",
    "full_shape = torch.ones(2, dtype=torch.float)\n",
    "size = torch.tensor([0.5,0.5], dtype=torch.float).view(-1,2) \n",
    "# batch = torch.tensor([0,0,1,2], dtype=torch.long)\n",
    "batch = None\n",
    "\n",
    "pos1 = torch.tensor([0.1,0.1,1, 0.2,0.2,1, 0.3,0.3,1, 0.1,0.9,1], dtype=torch.float).view(-1,3)\n",
    "pos2 = torch.tensor([0.6,0.6,1, 0.7,0.7,1, 0.8,0.8,1], dtype=torch.float).view(-1,3)\n",
    "\n",
    "print(fixed_voxel_grid(pos1[:, :2], full_shape, size, batch=batch))\n",
    "print('')\n",
    "print(fixed_voxel_grid(pos2[:, :2], full_shape, size, batch=batch))\n",
    "\n",
    "pos3 = torch.cat([pos1, pos2])\n",
    "batch1 = torch.tensor([0,0,0,0,1,1,1], dtype=torch.long)\n",
    "print(fixed_voxel_grid(pos3[:, :2], full_shape, size, batch=batch1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3250, 1], pos=[3250, 3], file_id='sequence_1', label=[1], y=[1], edge_index=[2, 66166])\n",
      "g=DataBatch(x=[7, 1], edge_index=[2, 6], pos=[7, 3], batch=[7], ptr=[3])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'img_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 131\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mout=\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mout\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    129\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m--> 131\u001b[0m net  \u001b[39m=\u001b[39m Net()\n\u001b[1;32m    132\u001b[0m net\u001b[39m.\u001b[39meval()\n\u001b[1;32m    134\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mg1:\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [5], line 81\u001b[0m, in \u001b[0;36mNet.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m0.5\u001b[39m,\u001b[39m0.5\u001b[39m])\n\u001b[1;32m     78\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfull_shape \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m1.0\u001b[39m,\u001b[39m1.0\u001b[39m])\n\u001b[0;32m---> 81\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool \u001b[39m=\u001b[39m MaxPoolingX(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_grid)\n\u001b[1;32m     83\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc \u001b[39m=\u001b[39m  Linear(\u001b[39m4\u001b[39m\u001b[39m*\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_grid), out_features\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'img_shape'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import aegnn\n",
    "import os\n",
    "from typing import Callable, List, Optional, Union\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "from torch_geometric.transforms import Cartesian\n",
    "from aegnn.models.layer import MaxPooling, MaxPoolingX\n",
    "\n",
    "from aegnn.models.layer import MaxPooling, MaxPoolingX\n",
    "from torch_geometric.nn.pool import max_pool_x, voxel_grid, avg_pool_x\n",
    "from torch_geometric.nn import GCNConv, Sequential, global_max_pool,global_mean_pool\n",
    "from torch.nn.functional import elu, relu\n",
    "from torch.nn import Dropout, Linear\n",
    "from torch_cluster import grid_cluster\n",
    "\n",
    "pl.seed_everything(12345)\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "device  = torch.device('cpu')\n",
    "\n",
    "\n",
    "\n",
    "x1 = torch.tensor([1.0, -2.0, 3.0, -4.0], dtype=torch.float, device=device).view(-1,1)\n",
    "x2 = torch.tensor([5.0, -6.0, 7.0,         ], dtype=torch.float, device=device).view(-1,1)\n",
    "\n",
    "edge1 = torch.tensor([0,2,1,3, 2,0,3,1], dtype=torch.long, device=device).view(2,-1)\n",
    "edge2 = torch.tensor([1,2,2,1], dtype=torch.long, device=device).view(2,-1)\n",
    "\n",
    "# in this setting, cluster will give diff result for g1,g2 and g\n",
    "# pos1 = torch.tensor([-0.49,-0.3,0.02, -0.49,-0.1,-0.03, 0.49,-0.1,0.01, 0.49,-0.3,-0.02], dtype=torch.float, device=device).view(-1,3)\n",
    "# pos1 += 0.5\n",
    "# pos2 = torch.tensor([-0.21,-0.49,0.02, -0.21,0.49,-0.03, -0.01,0.49,0.01,              ], dtype=torch.float, device=device).view(-1,3)\n",
    "# pos2 += 0.5\n",
    "\n",
    "# new test\n",
    "pos1 = torch.tensor([0.1,0.1,1, 0.2,0.2,1, 0.3,0.3,1, 0.1,0.9,1], dtype=torch.float, device=device).view(-1,3)\n",
    "pos2 = torch.tensor([0.6,0.6,1, 0.7,0.7,1, 0.8,0.8,1], dtype=torch.float, device=device).view(-1,3)\n",
    "\n",
    "g1 = Data(x=x1, edge_index=edge1, pos=pos1)\n",
    "g2 = Data(x=x2, edge_index=edge2, pos=pos2)\n",
    "g3 = Data(x=x2, edge_index=edge2, pos=pos2)\n",
    "g = torch_geometric.data.Batch.from_data_list([g1,g2])\n",
    "\n",
    "path1 = '/space/yyang22/datasets/data/storage/ncars/processed/training/sequence_0'\n",
    "path2 = '/space/yyang22/datasets/data/storage/ncars/processed/training/sequence_1'\n",
    "\n",
    "aegnn1 = torch.load(path2).to(device)\n",
    "aegnn2 = torch.load(path2).to(device)\n",
    "aegnn_whole = torch_geometric.data.Batch.from_data_list([aegnn1, aegnn2])\n",
    "print(aegnn1)\n",
    "\n",
    "print(f'g={g}')\n",
    "# print(g.x)\n",
    "# print(g.pos)\n",
    "# print(g.edge_index)\n",
    "# print(g.batch)\n",
    "\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(1, 4)\n",
    "        self.norm1 = BatchNorm(in_channels=4)\n",
    "        self.act   = elu\n",
    "\n",
    "        self.batch_size = 2\n",
    "        self.grid_div = 2\n",
    "        self.num_grid = self.grid_div*self.grid_div\n",
    "        # self.size=([1.0/self.grid_div,1.0/self.grid_div])\n",
    "        self.size = torch.tensor([0.5,0.5])\n",
    "        self.full_shape = torch.tensor([1.0,1.0])\n",
    "\n",
    "\n",
    "        self.pool = MaxPoolingX(self.size, size=self.num_grid)\n",
    "\n",
    "        self.fc =  Linear(4*(self.num_grid), out_features=2, bias=False)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x1 = data.x = self.conv1(data.x, data.edge_index)\n",
    "        # x2 = data.x = self.norm1(data.x)\n",
    "        x2 = data.x\n",
    "        # x3 = data.x = self.act(data.x)\n",
    "        x3 = data.x\n",
    "\n",
    "        # print(f'x1=\\n{x1}')\n",
    "        # print(f'x2=\\n{x2}')\n",
    "        # print(f'x3=\\n{x3}')\n",
    "\n",
    "        if data.batch is None:\n",
    "            data.batch = torch.zeros(data.num_nodes)\n",
    "        else:\n",
    "            print(f'data.batch=\\n{data.batch}')\n",
    "        \n",
    "        # end = ((data.batch.max().item() + 1.0)*self.num_grid - 1)\n",
    "        # print(f'end={end}')\n",
    "\n",
    "        # cluster = voxel_grid(data.pos[:, :2], batch=data.batch, size=self.size)\n",
    "        cluster = fixed_voxel_grid(data.pos[:, :2], full_shape=self.full_shape, batch=data.batch, size=self.size)\n",
    "\n",
    "        # pos = torch.cat([data.pos[:, :2], data.batch.unsqueeze(-1).type_as(data.pos[:, :2])], dim=-1)\n",
    "        # size = self.size + [1]\n",
    "        # print(size)\n",
    "        # size = torch.tensor(size, dtype=pos.dtype, device=pos.device)\n",
    "        # start = torch.tensor([0.0,0.0,0.0], dtype=pos.dtype, device=pos.device)\n",
    "        # end = torch.tensor([1.0,1.0,1.0], dtype=pos.dtype, device=pos.device)\n",
    "        # cluster = grid_cluster(pos, size, start, end)\n",
    "\n",
    "        x4_auth, _ = max_pool_x(cluster, data.x, data.batch, size=self.num_grid)\n",
    "\n",
    "        print(f'cluster=\\n{cluster}')\n",
    "        print(f'x4_auth=\\n{x4_auth}')\n",
    "\n",
    "        x4_aegnn = self.pool(data.x, pos=data.pos[:, :2], batch=data.batch)\n",
    "        # print(f'x4_aegnn=\\n{x4_aegnn}')\n",
    "        # print(f'same={torch.allclose(x4_auth, x4_aegnn)}')\n",
    "\n",
    "        x5 = x4_auth.view(-1, self.fc.in_features)\n",
    "        # print(f'x5=\\n{x5}')\n",
    "        out = self.fc(x5)\n",
    "        print(f'out=\\n{out}')\n",
    "\n",
    "        return out\n",
    "\n",
    "net  = Net()\n",
    "net.eval()\n",
    "\n",
    "print('\\ng1:')\n",
    "g1_out = net(g1)\n",
    "print('\\ng2:')\n",
    "g2_out = net(g2)\n",
    "print('\\ng:')\n",
    "g_out = net(g)\n",
    "\n",
    "# print('\\naegnn1:')\n",
    "# aegnn1_out = net(aegnn1)\n",
    "# print('\\naegnn2:')\n",
    "# aegnn2_out = net(aegnn2)\n",
    "# print('\\naegnn_whole:')\n",
    "# aegnn_whole_out = net(aegnn_whole)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 1, 4, 6, 6])\n",
      "tensor([[2.0000, 1.1000],\n",
      "        [4.0000, 3.1000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [5.0000, 5.1000],\n",
      "        [0.0000, 0.0000],\n",
      "        [7.0000, 7.1000],\n",
      "        [0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# from aegnn.models.layer import MaxPooling, MaxPoolingX\n",
    "from torch_geometric.nn.pool import max_pool_x, voxel_grid, avg_pool_x\n",
    "from torch_geometric.nn import GCNConv, Sequential, global_max_pool,global_mean_pool\n",
    "\n",
    "batch_size = 2\n",
    "grid_div = 2\n",
    "num_grid = grid_div*grid_div\n",
    "cluster = voxel_grid(g.pos[:, :2], batch=g.batch, size=([1.0/grid_div,1.0/grid_div]))\n",
    "x, _ = max_pool_x(cluster, g.x, g.batch, size=num_grid)\n",
    "# zero = torch.tensor([0,0,0,0,0,0,0])\n",
    "# x, _ = max_pool_x(zero, g.x, g.batch, size=num_grid)\n",
    "# x, _ = max_pool_x(cluster, g.x, g.batch)\n",
    "\n",
    "\n",
    "# x_new = x.view(batch_size, g.x.shape[1]*num_grid)\n",
    "\n",
    "print(cluster)\n",
    "print(x)\n",
    "# print(x_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.3892, 2.6133],\n",
      "        [8.5804, 4.4811]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Dropout, Linear, ReLU\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "pl.seed_everything(12345)\n",
    "\n",
    "fc = Linear(g.x.shape[1]*num_grid, out_features=2, bias=False)\n",
    "output = fc(x_new)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.5000, -0.5000],\n",
      "        [ 6.0000,  6.1000]])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv, Sequential, global_max_pool,global_mean_pool\n",
    "# gmp = global_max_pool\n",
    "gmp = global_mean_pool\n",
    "x_ori_output = gmp(g.x, batch=g.batch)\n",
    "print(x_ori_output)\n",
    "# fc_ori = Linear(g.x.shape[1], out_features=2, bias=False)\n",
    "# output_ori = fc_ori(x_ori_output)\n",
    "# print(x_ori_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[3, 1], edge_index=[2, 6], pos=[3, 3])\n",
      "tensor([[ 1.],\n",
      "        [ 3.],\n",
      "        [-4.]])\n",
      "tensor([[0, 0, 1, 1, 2, 2],\n",
      "        [1, 2, 0, 2, 0, 1]])\n",
      "tensor([[0.1500, 0.1500, 1.0000],\n",
      "        [0.1000, 0.6000, 1.0000],\n",
      "        [0.6000, 0.6000, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import aegnn\n",
    "import os\n",
    "from typing import Callable, List, Optional, Union\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "from torch_geometric.transforms import Cartesian\n",
    "from aegnn.models.layer import MaxPooling, MaxPoolingX\n",
    "\n",
    "from aegnn.models.layer import MaxPooling, MaxPoolingX\n",
    "from torch_geometric.nn.pool import max_pool_x, voxel_grid, avg_pool_x, max_pool\n",
    "from torch_geometric.nn import GCNConv, Sequential, global_max_pool,global_mean_pool\n",
    "from torch.nn.functional import elu, relu\n",
    "from torch.nn import Dropout, Linear\n",
    "from torch_cluster import grid_cluster\n",
    "\n",
    "pl.seed_everything(12345)\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "device  = torch.device('cpu')\n",
    "\n",
    "\n",
    "\n",
    "x1 = torch.tensor([1.0, -2.0, 3.0, -4.0], dtype=torch.float, device=device).view(-1,1)\n",
    "x2 = torch.tensor([5.0, -6.0, 7.0,         ], dtype=torch.float, device=device).view(-1,1)\n",
    "\n",
    "edge1 = torch.tensor([0,3,1,2,2,3, 3,0,2,1,3,2], dtype=torch.long, device=device).view(2,-1)\n",
    "edge2 = torch.tensor([1,2,2,1], dtype=torch.long, device=device).view(2,-1)\n",
    "\n",
    "pos1 = torch.tensor([0.1,0.1,1, 0.2,0.2,1, 0.1,0.6,1, 0.6,0.6,1], dtype=torch.float, device=device).view(-1,3)\n",
    "pos2 = torch.tensor([0.6,0.6,1, 0.7,0.7,1, 0.8,0.8,1], dtype=torch.float, device=device).view(-1,3)\n",
    "\n",
    "g1 = Data(x=x1, edge_index=edge1, pos=pos1)\n",
    "g2 = Data(x=x2, edge_index=edge2, pos=pos2)\n",
    "g3 = Data(x=x2, edge_index=edge2, pos=pos2)\n",
    "g = torch_geometric.data.Batch.from_data_list([g1,g2])\n",
    "\n",
    "batch_size = 2\n",
    "grid_div = 2\n",
    "num_grid = grid_div*grid_div\n",
    "cluster = voxel_grid(g1.pos[:, :2], batch=g1.batch, size=([1.0/grid_div,1.0/grid_div]))\n",
    "data = max_pool(cluster, g1)\n",
    "\n",
    "print(data)\n",
    "print(data.x)\n",
    "print(data.edge_index)\n",
    "print(data.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[4, 1], edge_index=[2, 6], pos=[4, 3])\n",
      "tensor([1.0000, 1.0000, 0.5831, 0.5831, 0.7071, 0.7071])\n",
      "edge_attr=tensor([], size=(6, 0))\n",
      "Data(x=[4, 1], edge_index=[2, 6], pos=[4, 3], edge_attr=[6, 0], edge_weight=[6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import aegnn\n",
    "import os\n",
    "from typing import Callable, List, Optional, Union\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "from torch_geometric.transforms import Cartesian, Distance\n",
    "from aegnn.models.layer import MaxPooling, MaxPoolingX\n",
    "\n",
    "from aegnn.models.layer import MaxPooling, MaxPoolingX\n",
    "from torch_geometric.nn.pool import max_pool_x, voxel_grid, avg_pool_x, max_pool\n",
    "from torch_geometric.nn import GCNConv, Sequential, global_max_pool,global_mean_pool\n",
    "from torch.nn.functional import elu, relu\n",
    "from torch.nn import Dropout, Linear\n",
    "from torch_cluster import grid_cluster\n",
    "\n",
    "pl.seed_everything(12345)\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "device  = torch.device('cpu')\n",
    "\n",
    "\n",
    "\n",
    "x1 = torch.tensor([1.0, -2.0, 3.0, -4.0], dtype=torch.float, device=device).view(-1,1)\n",
    "x2 = torch.tensor([5.0, -6.0, 7.0,         ], dtype=torch.float, device=device).view(-1,1)\n",
    "\n",
    "edge1 = torch.tensor([0,3,1,2,2,3, 3,0,2,1,3,2], dtype=torch.long, device=device).view(2,-1)\n",
    "edge2 = torch.tensor([1,2,2,1], dtype=torch.long, device=device).view(2,-1)\n",
    "\n",
    "pos1 = torch.tensor([0.1,0.1,1, 0.2,0.2,1, 0.1,0.6,1, 0.6,0.6,1], dtype=torch.float, device=device).view(-1,3)\n",
    "pos2 = torch.tensor([0.6,0.6,1, 0.7,0.7,1, 0.8,0.8,1], dtype=torch.float, device=device).view(-1,3)\n",
    "\n",
    "g1 = Data(x=x1, edge_index=edge1, pos=pos1)\n",
    "g2 = Data(x=x2, edge_index=edge2, pos=pos2)\n",
    "g3 = Data(x=x2, edge_index=edge2, pos=pos2)\n",
    "g = torch_geometric.data.Batch.from_data_list([g1,g2])\n",
    "\n",
    "print(g1)\n",
    "\n",
    "# trans2 = Cartesian()\n",
    "\n",
    "# g1 = trans2(g1)\n",
    "\n",
    "trans = Distance()\n",
    "g1 = trans(g1)\n",
    "g1.edge_weight = g1.edge_attr[:,-1]\n",
    "g1.edge_attr = g1.edge_attr[:, :-1]\n",
    "\n",
    "print(g1.edge_weight)\n",
    "print(f'edge_attr={g1.edge_attr}')\n",
    "print(g1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "CUDA_VISIBLE_DEVICES=2 wandb agent yyfteam/aegnn/l8y4mj6f\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.loadtxt('/users/yyang22/thesis/aegnn_project/aegnn_results/async_accuracy.csv', delimiter=',')\n",
    "plt.plot(data[:,0], data[:,1])\n",
    "\n",
    "plt.savefig('/users/yyang22/thesis/aegnn_project/aegnn_results/async_accuracy.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('aegnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9439450e489ce535473a2847795b2c81cbeeccb2f39d71287859ebd0392d6b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
