{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/yyang22/anaconda3/envs/aegnn/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 12345\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "import functools\n",
    "import glob\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import importlib as imp\n",
    "\n",
    "from tqdm import tqdm\n",
    "tprint = tqdm.write\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.pool import radius_graph\n",
    "from torch_geometric.transforms import FixedPoints\n",
    "from tqdm import tqdm\n",
    "from typing import Callable, Dict, List, Optional, Union\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "\n",
    "# from torch_geometric.nn.conv import PointNetConv\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.inits import reset\n",
    "from torch_geometric.typing import (\n",
    "    Adj,\n",
    "    OptTensor,\n",
    "    PairOptTensor,\n",
    "    PairTensor\n",
    ")\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "\n",
    "import aegnn\n",
    "from aegnn.models.networks.my_fuse import MyConvBNReLU\n",
    "\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Parameter as P\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "pl.seed_everything(12345)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "torch.set_printoptions(precision=4)\n",
    "\n",
    "def quant_tensor(real, scale, bit, signed):\n",
    "    if signed:\n",
    "        max = pow(2, bit-1) - 1\n",
    "        min = - max  # symmetric clamp\n",
    "    else:\n",
    "        max = pow(2, bit) - 1\n",
    "        min = 0\n",
    "\n",
    "    quant = torch.round(real/scale)\n",
    "    if torch.max(quant) > max:\n",
    "        print(f'overflow: max={torch.max(quant).item()}')\n",
    "    if torch.min(quant) < min:\n",
    "        print(f'underflow: min={torch.min(quant).item()}')\n",
    "    quant = torch.clamp(quant, min=min, max=max)\n",
    "\n",
    "    return quant\n",
    "\n",
    "def dequant_tensor(quant, scale):\n",
    "    real = quant * scale\n",
    "    return real\n",
    "\n",
    "\n",
    "def q(tensor, bit, signed, replace_scale = None):\n",
    "    t_max = torch.max(tensor)\n",
    "    t_min = torch.min(tensor)\n",
    "    t_abs_max = torch.maximum(torch.abs(t_max), torch.abs(t_min))\n",
    "\n",
    "    if replace_scale is not None:\n",
    "        scale = replace_scale\n",
    "    else:\n",
    "        if signed:\n",
    "            q_max = pow(2, bit-1) - 1\n",
    "            q_min = -q_max\n",
    "            scale = 2*t_abs_max / (q_max-q_min)\n",
    "        else:\n",
    "            q_max = pow(2, bit) - 1\n",
    "            q_min = 0\n",
    "            scale = (t_abs_max - 0.0) / (q_max-q_min)\n",
    "    qtensor = quant_tensor(tensor, scale=scale, bit=bit, signed=signed)\n",
    "\n",
    "    dqtensor = dequant_tensor(qtensor, scale)\n",
    "\n",
    "    return qtensor, scale, dqtensor\n",
    "\n",
    "def reg(fc, w, b):\n",
    "    fc.weight = P(w)\n",
    "    fc.bias = P(b)\n",
    "    return fc\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 = \n",
      "tensor([[0.1759, 0.8837],\n",
      "        [0.4747, 0.1197]])\n",
      "\n",
      "x1 = \n",
      "tensor([[ 0.5631,  0.0649, -0.1187],\n",
      "        [ 0.2499, -0.0433, -0.6513]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "xr1 = \n",
      "tensor([[0.5631, 0.0649, 0.0000],\n",
      "        [0.2499, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "x2 = \n",
      "tensor([[ 0.4669, -0.6187, -0.6027, -0.2498],\n",
      "        [ 0.2995, -0.4151, -0.4490, -0.4279]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "y = \n",
      "tensor([[0.4669, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2995, 0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "tensor([[0.1759, 0.8837],\n",
      "        [0.4747, 0.1197]])\n",
      "tensor([[0.1767, 0.8837],\n",
      "        [0.4748, 0.1213]])\n",
      "\n",
      "tensor([[0.5631, 0.0649, 0.0000],\n",
      "        [0.2499, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.5631, 0.0640, 0.0000],\n",
      "        [0.2495, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n",
      "\n",
      "tensor([[0.4669, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2995, 0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.4669, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3003, 0.0000, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n",
      "\n",
      "tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "tensor(0.0054, grad_fn=<DivBackward0>)\n",
      "qfc1.weight = \n",
      "Parameter containing:\n",
      "tensor([[-96.,  41.],\n",
      "        [-49.,   8.],\n",
      "        [-17., 127.]], requires_grad=True)\n",
      "d qfc1.weight = \n",
      "tensor([[-0.5002,  0.2136],\n",
      "        [-0.2553,  0.0417],\n",
      "        [-0.0886,  0.6617]], grad_fn=<MulBackward0>)\n",
      "fc1.weight = \n",
      "Parameter containing:\n",
      "tensor([[-0.4994,  0.2146],\n",
      "        [-0.2575,  0.0410],\n",
      "        [-0.0906,  0.6617]], requires_grad=True)\n",
      "\n",
      "qfc1.bias = \n",
      "Parameter containing:\n",
      "tensor([ 25547.,   4097., -38077.], requires_grad=True)\n",
      "d qfc1.weight = \n",
      "tensor([ 0.4613,  0.0740, -0.6875], grad_fn=<MulBackward0>)\n",
      "fc1.bias = \n",
      "Parameter containing:\n",
      "tensor([ 0.4613,  0.0740, -0.6875], requires_grad=True)\n",
      "\n",
      "qfc2.weight = \n",
      "Parameter containing:\n",
      "tensor([[ 101.,   88.,   44.],\n",
      "        [-127.,  -85.,   68.],\n",
      "        [-121.,   55.,   15.],\n",
      "        [ 117.,   44.,   68.]], requires_grad=True)\n",
      "d qfc2.weight = \n",
      "tensor([[ 0.4538,  0.3954,  0.1977],\n",
      "        [-0.5706, -0.3819,  0.3055],\n",
      "        [-0.5436,  0.2471,  0.0674],\n",
      "        [ 0.5257,  0.1977,  0.3055]], grad_fn=<MulBackward0>)\n",
      "fc2.weight = \n",
      "Parameter containing:\n",
      "tensor([[ 0.4523,  0.3960,  0.1962],\n",
      "        [-0.5706, -0.3836,  0.3039],\n",
      "        [-0.5417,  0.2465,  0.0666],\n",
      "        [ 0.5277,  0.1977,  0.3050]], requires_grad=True)\n",
      "\n",
      "qfc2.bias = \n",
      "Parameter containing:\n",
      "tensor([ 18795., -27469., -31615., -56425.], requires_grad=True)\n",
      "d qfc2.weight = \n",
      "tensor([ 0.1865, -0.2725, -0.3137, -0.5598], grad_fn=<MulBackward0>)\n",
      "fc2.bias = \n",
      "Parameter containing:\n",
      "tensor([ 0.1865, -0.2725, -0.3137, -0.5598], requires_grad=True)\n",
      "\n",
      "qx0 = \n",
      "tensor([[ 51., 255.],\n",
      "        [137.,  35.]])\n",
      "\n",
      "c_qx1 = \n",
      "tensor([[ 254.,   30.,  -54.],\n",
      "        [ 113.,  -19., -294.]], grad_fn=<RoundBackward0>)\n",
      "\n",
      "c_qxr1 = \n",
      "tensor([[254.,  30.,   0.],\n",
      "        [113.,   0.,   0.]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "c_qx2 = \n",
      "tensor([[ 255., -337., -329., -138.],\n",
      "        [ 164., -227., -245., -234.]], grad_fn=<RoundBackward0>)\n",
      "\n",
      "c_qy = \n",
      "tensor([[255.,   0.,   0.,   0.],\n",
      "        [164.,   0.,   0.,   0.]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "dc_qy = \n",
      "tensor([[0.4669, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3003, 0.0000, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n",
      "\n",
      "y = \n",
      "tensor([[0.4669, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2995, 0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test pass\n",
    "fc1 = Linear(2,3)\n",
    "fc2 = Linear(3,4)\n",
    "qfc1 = Linear(2,3)\n",
    "qfc2 = Linear(3,4)\n",
    "\n",
    "x0 = torch.rand(2,2)\n",
    "print(f'x0 = \\n{x0}\\n')\n",
    "x1 = fc1(x0)\n",
    "print(f'x1 = \\n{x1}\\n')\n",
    "xr1 = relu(x1)\n",
    "print(f'xr1 = \\n{xr1}\\n')\n",
    "x2 = fc2(xr1)\n",
    "print(f'x2 = \\n{x2}\\n')\n",
    "y = relu(x2)\n",
    "print(f'y = \\n{y}\\n')\n",
    "\n",
    "qx0, x0_scale, dqx0 = q(x0, bit=8, signed=False)\n",
    "qw1, w1_scale, dqw1 = q(fc1.weight, bit=8, signed=True)\n",
    "qb1, b1_scale, dqb1 = q(fc1.bias, bit=32, signed=True, replace_scale=x0_scale*w1_scale)\n",
    "qfc1 = reg(qfc1, qw1, qb1)\n",
    "print(x0)\n",
    "print(dqx0)\n",
    "print('')\n",
    "\n",
    "qxr1, xr1_scale, dqxr1 = q(xr1, bit=8, signed=False)\n",
    "qw2, w2_scale, dqw2 = q(fc2.weight, bit=8, signed=True)\n",
    "qb2, b2_scale, dqb2 = q(fc2.bias, bit=32, signed=True, replace_scale=xr1_scale*w2_scale)\n",
    "qfc2 = reg(qfc2, qw2, qb2)\n",
    "print(xr1)\n",
    "print(dqxr1)\n",
    "print('')\n",
    "\n",
    "qy, y_scale, dqy = q(y, bit=8, signed=False)\n",
    "print(y)\n",
    "print(dqy)\n",
    "print('')\n",
    "\n",
    "M1 = x0_scale*w1_scale/xr1_scale\n",
    "M2 = xr1_scale*w2_scale/y_scale\n",
    "print(M1)\n",
    "print(M2)\n",
    "\n",
    "print(f'qfc1.weight = \\n{qfc1.weight}')\n",
    "dw1 = dequant_tensor(qfc1.weight, scale=w1_scale)\n",
    "print(f'd qfc1.weight = \\n{dw1}')\n",
    "print(f'fc1.weight = \\n{fc1.weight}')\n",
    "print('')\n",
    "print(f'qfc1.bias = \\n{qfc1.bias}')\n",
    "db1 = dequant_tensor(qfc1.bias, scale=b1_scale)\n",
    "print(f'd qfc1.weight = \\n{db1}')\n",
    "print(f'fc1.bias = \\n{fc1.bias}')\n",
    "print('')\n",
    "\n",
    "print(f'qfc2.weight = \\n{qfc2.weight}')\n",
    "dw2 = dequant_tensor(qfc2.weight, scale=w2_scale)\n",
    "print(f'd qfc2.weight = \\n{dw2}')\n",
    "print(f'fc2.weight = \\n{fc2.weight}')\n",
    "print('')\n",
    "print(f'qfc2.bias = \\n{qfc2.bias}')\n",
    "db2 = dequant_tensor(qfc2.bias, scale=b2_scale)\n",
    "print(f'd qfc2.weight = \\n{db2}')\n",
    "print(f'fc2.bias = \\n{fc2.bias}')\n",
    "print('')\n",
    "\n",
    "print(f'qx0 = \\n{qx0}\\n')\n",
    "c_qx1 = torch.round(qfc1(qx0)*M1)\n",
    "print(f'c_qx1 = \\n{c_qx1}\\n')\n",
    "c_qxr1 = relu(c_qx1)\n",
    "print(f'c_qxr1 = \\n{c_qxr1}\\n')\n",
    "\n",
    "c_qx2 = torch.round(qfc2(c_qxr1)*M2)\n",
    "print(f'c_qx2 = \\n{c_qx2}\\n')\n",
    "c_qy = relu(c_qx2)\n",
    "print(f'c_qy = \\n{c_qy}\\n')\n",
    "\n",
    "dc_qy = dequant_tensor(c_qy, scale=y_scale)\n",
    "print(f'dc_qy = \\n{dc_qy}\\n')\n",
    "print(f'y = \\n{y}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos_dim = 2\n",
    "        self.lin1 = Linear(2+self.pos_dim,3)\n",
    "        self.qlin1 = Linear(2+self.pos_dim,3)\n",
    "\n",
    "        self.lin2 = Linear(3+self.pos_dim,4)\n",
    "        self.qlin2 = Linear(3+self.pos_dim,4)\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    def forward(self, x0, CONST):\n",
    "        self.x0 = x0\n",
    "        self.CONST = CONST\n",
    "        self.x0_c = torch.cat([self.x0, self.CONST], dim=1)\n",
    "        self.x1 = relu(self.lin1(self.x0_c))\n",
    "\n",
    "        self.x1_c = torch.cat([self.x1, self.CONST], dim=1)\n",
    "        self.x2 = relu(self.lin2(self.x1_c))\n",
    "\n",
    "        print(f'x0 = \\n{self.x0}')\n",
    "        print(f'x0_c = \\n{self.x0_c}')\n",
    "        print(f'x1 = \\n{self.x1}')\n",
    "        print(f'x1_c = \\n{self.x1_c}')\n",
    "        print(f'x2 = \\n{self.x2}\\n')\n",
    "\n",
    "        return self.x2\n",
    "\n",
    "    def quantize(self):\n",
    "        # self.dpos_scale = 4.0 / 255\n",
    "        # self.dpos_scale = 4.0 / 2048  # 11bit = 8bit +3\n",
    "        self.dpos_scale = 4.0 / pow(2,19)\n",
    "\n",
    "\n",
    "        self.qx0, self.x0_scale, self.dqx0 = q(self.x0, bit=16, signed=False)\n",
    "\n",
    "\n",
    "        # self.qw1, self.w1_scale, self.dqw1 = q(self.lin1.weight, bit=8, signed=True)\n",
    "        x0_max = torch.max(self.x0)\n",
    "        wx1_max = torch.max(self.lin1.weight[:, :-self.pos_dim])\n",
    "        wx1_min = torch.min(self.lin1.weight[:, :-self.pos_dim])\n",
    "        wx1_abs_max = torch.maximum(torch.abs(wx1_max), torch.abs(wx1_min))\n",
    "        self.wx1_scale = 2 * wx1_abs_max / (32767 - (-32767)) #int16\n",
    "\n",
    "        wpos1_max = torch.max(self.lin1.weight[:, -self.pos_dim:])\n",
    "        wpos1_min = torch.min(self.lin1.weight[:, -self.pos_dim:])\n",
    "        wpos1_abs_max = torch.maximum(torch.abs(wpos1_max), torch.abs(wpos1_min))\n",
    "        print(f'wpos1_abs_max = {wpos1_abs_max}')\n",
    "        # wpos psedo max is only for dpos part of weight. Though dpos_max = arg.radius = 3. Here take 4 for fast calculation\n",
    "        # wpos1_pseudo_max = wx1_abs_max * x0_max / 4.0\n",
    "        wpos1_pseudo_max = wx1_abs_max * x0_max * 2.0\n",
    "        print(f'wpos1_pseudo_max = {wpos1_pseudo_max}')\n",
    "        self.wpos1_scale = 2 * wpos1_pseudo_max / (32767 - (-32767))\n",
    "\n",
    "        print(f'quant wx')\n",
    "        self.wx1_quant = quant_tensor(self.lin1.weight[:, :-self.pos_dim], scale=self.wx1_scale, bit=16, signed=True)\n",
    "        print(f'quant wpos')\n",
    "        self.wpos1_quant = quant_tensor(self.lin1.weight[:, -self.pos_dim:], scale=self.wpos1_scale, bit=16, signed=True)\n",
    "        self.w1_quant = torch.cat([self.wx1_quant, self.wpos1_quant], dim=1)\n",
    "\n",
    "        self.qb1, self.b1_scale, self.dqb1 = q(self.lin1.bias, bit=32, signed=True, replace_scale=self.x0_scale*self.wx1_scale)\n",
    "        reg(self.qlin1, w=self.w1_quant, b=self.qb1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.qx1, self.x1_scale, self.dqx1 = q(self.x1, bit=16, signed=False)\n",
    "        self.M1 = self.x0_scale*self.wx1_scale / self.x1_scale\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # self.qw2, self.w2_scale, self.dqw2 = q(self.lin2.weight, bit=8, signed=True)\n",
    "\n",
    "\n",
    "        x1_max = torch.max(self.x1)\n",
    "        wx2_max = torch.max(self.lin2.weight[:, :-self.pos_dim])\n",
    "        wx2_min = torch.min(self.lin2.weight[:, :-self.pos_dim])\n",
    "        wx2_abs_max = torch.maximum(torch.abs(wx2_max), torch.abs(wx2_min))\n",
    "        self.wx2_scale = 2 * wx2_abs_max / (32767 - (-32767))\n",
    "\n",
    "        wpos2_max = torch.max(self.lin2.weight[:, -self.pos_dim:])\n",
    "        wpos2_min = torch.min(self.lin2.weight[:, -self.pos_dim:])\n",
    "        wpos2_abs_max = torch.maximum(torch.abs(wpos2_max), torch.abs(wpos2_min))\n",
    "        print(f'wpos2_abs_max = {wpos2_abs_max}')\n",
    "        # wpos psedo max is only for dpos part of weight. Though dpos_max = arg.radius = 3. Here take 4 for fast calculation\n",
    "        # wpos2_pseudo_max = wx2_abs_max * x0_max / 4.0\n",
    "        wpos2_pseudo_max = wx2_abs_max * x1_max * 2.0\n",
    "        print(f'wpos2_pseudo_max = {wpos2_pseudo_max}')\n",
    "        self.wpos2_scale = 2 * wpos2_pseudo_max / (32767 - (-32767))\n",
    "\n",
    "        print(f'quant wx')\n",
    "        self.wx2_quant = quant_tensor(self.lin2.weight[:, :-self.pos_dim], scale=self.wx2_scale, bit=16, signed=True)\n",
    "        print(f'quant wpos')\n",
    "        self.wpos2_quant = quant_tensor(self.lin2.weight[:, -self.pos_dim:], scale=self.wpos2_scale, bit=16, signed=True)\n",
    "        self.w2_quant = torch.cat([self.wx2_quant, self.wpos2_quant], dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.qb2, self.b2_scale, self.dqb2 = q(self.lin2.bias, bit=32, signed=True, replace_scale=self.x1_scale*self.wx2_scale)\n",
    "        reg(self.qlin2, w=self.w2_quant, b=self.qb2)\n",
    "\n",
    "        self.qx2, self.x2_scale, self.dqx2 = q(self.x2, bit=16, signed=False)\n",
    "        self.M2 = self.x1_scale*self.wx2_scale / self.x2_scale\n",
    "\n",
    "    def qforward(self):\n",
    "        print(f'\\nqforward start\\n')\n",
    "        self.qc = quant_tensor(self.CONST, scale=self.dpos_scale, bit=19, signed=False)\n",
    "        self.c_qx0_c = torch.cat([self.qx0, self.qc], dim=1)\n",
    "        self.c_qx1 = torch.round(relu(self.M1 * self.qlin1(self.c_qx0_c)))\n",
    "        # self.c_qx2 = torch.round(relu(self.M2 * self.qlin2(self.c_qx1)))\n",
    "        self.c_qx1_c = torch.cat([self.c_qx1, self.qc], dim=1)\n",
    "        self.c_qx2 = torch.round(relu(self.M2 * self.qlin2(self.c_qx1_c)))\n",
    "\n",
    "        # self.dc_qx0 = dequant_tensor(self.c_qx0, scale=self.x0_scale)\n",
    "        self.dc_qx1 = dequant_tensor(self.c_qx1, scale=self.x1_scale)\n",
    "        self.dc_qx2 = dequant_tensor(self.c_qx2, scale=self.x2_scale)\n",
    "\n",
    "        print(f'c_qx0_c = \\n{self.c_qx0_c}')\n",
    "        # print(f'dc_qx0 = \\n{self.dc_qx0}')\n",
    "        print(f'x0 = \\n{self.x0}\\n')\n",
    "        print(f'c_qx1 = \\n{self.c_qx1}')\n",
    "        print(f'dc_qx1 = \\n{self.dc_qx1}')\n",
    "        print(f'x1 = \\n{self.x1}\\n')\n",
    "        print(f'c_qx2 = \\n{self.c_qx2}')\n",
    "        print(f'dc_qx2 = \\n{self.dc_qx2}')\n",
    "        print(f'x2 = \\n{self.x2}\\n')\n",
    "\n",
    "        # return self.dc_qx2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 = \n",
      "tensor([[0.1975, 0.9458],\n",
      "        [0.6762, 0.7570]])\n",
      "x0_c = \n",
      "tensor([[0.1975, 0.9458, 3.0000, 0.0000],\n",
      "        [0.6762, 0.7570, 0.0000, 3.0000]])\n",
      "x1 = \n",
      "tensor([[0.0000, 1.2900, 1.0098],\n",
      "        [0.2307, 0.1481, 0.1677]])\n",
      "x1_c = \n",
      "tensor([[0.0000, 1.2900, 1.0098, 3.0000, 0.0000],\n",
      "        [0.2307, 0.1481, 0.1677, 0.0000, 3.0000]])\n",
      "x2 = \n",
      "tensor([[0.4949, 1.9529, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6714, 1.1601]])\n",
      "\n",
      "wpos1_abs_max = 0.34854263067245483\n",
      "wpos1_pseudo_max = 0.8938669562339783\n",
      "quant wx\n",
      "quant wpos\n",
      "wpos2_abs_max = 0.43047958612442017\n",
      "wpos2_pseudo_max = 1.1475560665130615\n",
      "quant wx\n",
      "quant wpos\n",
      "\n",
      "qforward start\n",
      "\n",
      "c_qx0_c = \n",
      "tensor([[ 13687.,  65535., 393216.,      0.],\n",
      "        [ 46855.,  52456.,      0., 393216.]])\n",
      "x0 = \n",
      "tensor([[0.1975, 0.9458],\n",
      "        [0.6762, 0.7570]])\n",
      "\n",
      "c_qx1 = \n",
      "tensor([[    0., 65537., 51302.],\n",
      "        [11720.,  7522.,  8519.]])\n",
      "dc_qx1 = \n",
      "tensor([[0.0000, 1.2900, 1.0098],\n",
      "        [0.2307, 0.1481, 0.1677]])\n",
      "x1 = \n",
      "tensor([[0.0000, 1.2900, 1.0098],\n",
      "        [0.2307, 0.1481, 0.1677]])\n",
      "\n",
      "c_qx2 = \n",
      "tensor([[16610., 65537.,     0.,     0.],\n",
      "        [    0.,     0., 22529., 38934.]])\n",
      "dc_qx2 = \n",
      "tensor([[0.4950, 1.9530, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6714, 1.1602]])\n",
      "x2 = \n",
      "tensor([[0.4949, 1.9529, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6714, 1.1601]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x0 = torch.rand(2,2)\n",
    "    c = torch.tensor([[3.0,0.0],[0.0,3.0]])\n",
    "    net = QNet()\n",
    "\n",
    "    x2 = net(x0,c)\n",
    "\n",
    "    net.quantize()\n",
    "    net.qforward()\n",
    "    # dc_qx2 = net.qforward()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aegnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
